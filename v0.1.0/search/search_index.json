{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Docs Overview","text":""},{"location":"#getting-started","title":"Getting Started","text":"<p>The \"Getting Started\" section is where you'll learn how to begin using Claudie. We'll guide you through the initial steps and show you how to set things up, so you can start using the software right away. </p> <p>You'll also find helpful information on how to customize Claudie to suit your needs, including specifications for the settings you can adjust, and examples of how to use configuration files to get started.</p> <p>By following the steps in this section, you'll have everything you need to start using Claudie with confidence!</p>"},{"location":"#how-claudie-works","title":"How Claudie works","text":"<p>In this section, we'll show you how Claudie works and guide you through our workflow. We'll explain how we store and manage data, balance the workload across different parts of the system, and automatically adjust resources to handle changes in demand.</p> <p>By following our explanations, you'll gain a better understanding of how Claudie operates and be better equipped to use it effectively.</p>"},{"location":"#claudie-use-cases","title":"Claudie Use Cases","text":"<p>The \"Claudie Use Cases\" section includes examples of different ways you can use Claudie to solve various problems. We've included these examples to help you understand the full range of capabilities Claudie offers and to show you how it can be applied in different scenarios. </p> <p>By exploring these use cases, you'll get a better sense of how Claudie can be a valuable tool for your work.</p>"},{"location":"#roadmap-for-claudie","title":"Roadmap for Claudie","text":"<p>In this section, you'll find a roadmap for Claudie that outlines the features we've already added and those we plan to add in the future.</p> <p>By checking out the roadmap, you'll be able to stay informed about the latest updates and see how Claudie is evolving to meet the needs of its users.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>In this section, we've gathered all the information you'll need if you want to help contribute to the Claudie project or release a new version of the software. </p> <p>By checking out this section, you'll get a better sense of what's involved in contributing and how you can be part of making Claudie even better.</p>"},{"location":"#changelog","title":"CHANGELOG","text":"<p>The \"CHANGELOG\" section is where you can find information about all the changes, updates, and issues related to each version of Claudie. </p>"},{"location":"CHANGELOG/changelog-0.1.x/","title":"Claudie <code>v0.1</code>","text":"<p>The first official release of Claudie</p>"},{"location":"CHANGELOG/changelog-0.1.x/#deployment","title":"Deployment","text":"<p>To deploy the Claudie <code>v0.1.X</code>, please:</p> <ol> <li> <p>Download the archive and checksums from the release page</p> </li> <li> <p>Verify the archive with the <code>sha256</code> (optional)</p> <pre><code>sha256sum -c --ignore-missing checksums.txt\n</code></pre> <p>If valid, output is, depending on the archive downloaded</p> <pre><code>claudie.tar.gz: OK\n</code></pre> <p>or</p> <pre><code>claudie.zip: OK\n</code></pre> <p>or both.</p> </li> <li> <p>Lastly, unpack the archive and deploy using <code>kubectl</code></p> <p>We strongly recommend changing the default credentials for MongoDB, MinIO and DynamoDB before you deploy it. To do this, change contents of the files in <code>mongo/secrets</code>, <code>minio/secrets</code> and <code>dynamo/secrets</code> respectively.</p> <pre><code>kubectl apply -k .\n</code></pre> </li> </ol>"},{"location":"CHANGELOG/changelog-0.1.x/#v010","title":"v0.1.0","text":""},{"location":"CHANGELOG/changelog-0.1.x/#features","title":"Features","text":"<ul> <li>Multi-cloud kubernetes cluster management</li> <li>Multi-cloud loadbalancer management</li> <li>Fast scale-up/scale-down of defined infrastructure</li> <li>Persistent storage via Longhorn</li> <li>Support for AWS, Azure, GCP, OCI and Hetzner</li> <li>GCP DNS zone support</li> <li>Claudie deployment on <code>amd64</code> and <code>arm64</code> clusters</li> </ul>"},{"location":"CHANGELOG/changelog-0.1.x/#bugfixes","title":"Bugfixes","text":"<ul> <li>As this is first release there are no bugfixes</li> </ul>"},{"location":"CHANGELOG/changelog-0.1.x/#known-issues","title":"Known issues","text":"<ul> <li><code>iptables</code> reset after reboot and block all traffic on <code>OCI</code> node #466</li> <li>Occasional connection issues between Claudie created clusters and Claudie on Hetzner and GCP #276</li> <li>Unable to easily recover after error #528</li> </ul>"},{"location":"claudie-workflow/claudie-workflow/","title":"Claudie","text":""},{"location":"claudie-workflow/claudie-workflow/#a-single-platform-for-multiple-clouds","title":"A single platform for multiple clouds","text":""},{"location":"claudie-workflow/claudie-workflow/#microservices","title":"Microservices","text":"<ul> <li>Context-box</li> <li>Scheduler</li> <li>Builder</li> <li>Terraformer</li> <li>Ansibler</li> <li>Kube-eleven</li> <li>Kuber</li> <li>Frontend</li> </ul>"},{"location":"claudie-workflow/claudie-workflow/#data-stores","title":"Data stores","text":"<ul> <li>MongoDB</li> <li>Minio</li> </ul>"},{"location":"claudie-workflow/claudie-workflow/#tools-used","title":"Tools used","text":"<ul> <li>Terraform</li> <li>Ansible</li> <li>KubeOne</li> <li>Longhorn</li> <li>Nginx</li> <li>Calico</li> <li>K8s-sidecar</li> <li>gRPC</li> </ul>"},{"location":"claudie-workflow/claudie-workflow/#context-box","title":"Context-box","text":"<p>Context box is Claudie's \"control unit\". It holds pending configs, which need to be processed, periodically checks for new/changed configs and receives new configs from <code>frontend</code>.</p>"},{"location":"claudie-workflow/claudie-workflow/#api","title":"API","text":"<pre><code>  // Saves the config parsed by Frontend.\nrpc SaveConfigFrontEnd(SaveConfigRequest) returns (SaveConfigResponse);\n// Saves the config parsed by Scheduler.\nrpc SaveConfigScheduler(SaveConfigRequest) returns (SaveConfigResponse);\n// Saves the config parsed by Builder.\nrpc SaveConfigBuilder(SaveConfigRequest) returns (SaveConfigResponse);\n// Gets a single config from the database.\nrpc GetConfigFromDB(GetConfigFromDBRequest) returns (GetConfigFromDBResponse);\n// *(NEEDS DELETION)*\nrpc GetConfigByName(GetConfigByNameRequest) returns (GetConfigByNameResponse);\n// Gets a config from Scheduler's queue of pending configs.\nrpc GetConfigScheduler(GetConfigRequest) returns (GetConfigResponse);\n// Gets a config from Builder's queue of pending configs.\nrpc GetConfigBuilder(GetConfigRequest) returns (GetConfigResponse);\n// Gets all configs from the database.\nrpc GetAllConfigs(GetAllConfigsRequest) returns (GetAllConfigsResponse);\n// Sets the manifest to null, effectively forcing the deletion of the infrastructure\n// defined by the manifest on the very next config (diff-) check.\nrpc DeleteConfig(DeleteConfigRequest) returns (DeleteConfigResponse);\n// Deletes the config from the database.\nrpc DeleteConfigFromDB(DeleteConfigRequest) returns (DeleteConfigResponse);\n</code></pre>"},{"location":"claudie-workflow/claudie-workflow/#flow","title":"Flow","text":"<ul> <li>Receives a <code>config</code> from Frontend, calculates its <code>msChecksum</code> and saves it to the database</li> <li>Periodically checks for <code>config</code> changes and pushes the <code>config</code> to the <code>schedulerQueue</code> if <code>msChecksum</code> != <code>dsChecksum</code></li> <li>Periodically checks for <code>config</code> changes and pushes the <code>config</code> to the <code>builderQueue</code> if <code>dsChecksum</code> != <code>csChecksum</code></li> <li>Receives a <code>config</code> with the <code>desiredState</code> from Scheduler and saves it to the database</li> <li>Receives a <code>config</code> with the <code>currentState</code> from Builder and saves it to the database</li> </ul>"},{"location":"claudie-workflow/claudie-workflow/#variables-used","title":"Variables used","text":"variable meaning <code>msChecksum</code> manifest checksum <code>dsChecksum</code> desired state checksum <code>csChecksum</code> current state checksum"},{"location":"claudie-workflow/claudie-workflow/#scheduler","title":"Scheduler","text":"<p>Scheduler brings the infrastructure to a desired the state based on the manifest contained in the config that is received from Context-box.</p> <p>Scheduler also monitors the health of current infrastructure and manages any operations based on actual health state (e.g. replacement of broken nodes, etc. [work in progress]).</p>"},{"location":"claudie-workflow/claudie-workflow/#api_1","title":"API","text":"<pre><code>This service is a gRPC client, thus it does not provide any API\n</code></pre>"},{"location":"claudie-workflow/claudie-workflow/#flow_1","title":"Flow","text":"<ul> <li>Periodically pulls <code>config</code> from Context-Box's <code>schedulerQueue</code></li> <li>Creates <code>desiredState</code> with <code>dsChecksum</code> based on the <code>config</code></li> <li>Sends the <code>config</code> file back to Context-box</li> </ul>"},{"location":"claudie-workflow/claudie-workflow/#builder","title":"Builder","text":"<p>Builder aligns the current state of the infrastructure with the desired state. It calls methods on <code>terraformer</code>, <code>ansibler</code>, <code>kube-eleven</code> and <code>kuber</code> in order to manage the infrastructure. It follows that Builder also takes care of deleting nodes from a kubernetes cluster by finding differences between <code>desiredState</code> and <code>currentState</code>.</p>"},{"location":"claudie-workflow/claudie-workflow/#api_2","title":"API","text":"<pre><code>This service is a gRPC client, thus it does not provide any API\n</code></pre>"},{"location":"claudie-workflow/claudie-workflow/#flow_2","title":"Flow","text":"<ul> <li>Periodically polls Context-Box's <code>builderQueue</code> for changes in <code>config</code>, pulls it when changed</li> <li>Calls Terraformer, Ansibler, Kube-eleven and Kuber</li> <li>Creates <code>currentState</code></li> <li>Sends updated <code>config</code> with the <code>currentState</code> to Context-box</li> </ul>"},{"location":"claudie-workflow/claudie-workflow/#terraformer","title":"Terraformer","text":"<p>Terraformer creates or destroys infrastructure (specified in the desired state) via Terraform calls.</p>"},{"location":"claudie-workflow/claudie-workflow/#api_3","title":"API","text":"<pre><code>  // Builds the infrastructure based on the provided desired state (includes addition/deletion of *stuff*).\nrpc BuildInfrastructure(BuildInfrastructureRequest) returns (BuildInfrastructureResponse);\n// Destroys the infrastructure completely.\nrpc DestroyInfrastructure(DestroyInfrastructureRequest) returns (DestroyInfrastructureResponse);\n</code></pre>"},{"location":"claudie-workflow/claudie-workflow/#flow_3","title":"Flow","text":"<ul> <li>Receives a <code>config</code> from Builder</li> <li>Uses Terraform to create infrastructure based on the <code>desiredState</code></li> <li>Updates the <code>currentState</code> in the <code>config</code></li> <li>Upon receiving a deletion request, Terraformer destroys the infrastructure based on the current state</li> </ul>"},{"location":"claudie-workflow/claudie-workflow/#ansibler","title":"Ansibler","text":"<p>Ansibler uses Ansible to:   - set up Wireguard VPN between the nodes   - set up nginx load balancer   - install dependencies for nodes in a kubernetes cluster</p>"},{"location":"claudie-workflow/claudie-workflow/#api_4","title":"API","text":"<pre><code>  // InstallNodeRequirements installs any requirements there are on all of the nodes.\nrpc InstallNodeRequirements(InstallRequest) returns (InstallResponse);\n// InstallVPN sets up a VPN between the nodes in the k8s cluster and lb clusters.\nrpc InstallVPN(InstallRequest) returns (InstallResponse);\n// SetUpLoadbalancers sets up the load balancers, the DNS and verifies their configuration.\nrpc SetUpLoadbalancers(SetUpLBRequest) returns (SetUpLBResponse);\n// TeardownLoadBalancers correctly destroys the load balancers attached to a k8s\n// cluster by choosing a new ApiServer endpoint.\nrpc TeardownLoadBalancers(TeardownLBRequest) returns (TeardownLBResponse);\n</code></pre>"},{"location":"claudie-workflow/claudie-workflow/#flow_4","title":"Flow","text":"<ul> <li>Receives a <code>configToDelete</code> from Builder for <code>TeardownLoadBalancers()</code></li> <li>Finds the new ApiEndpoint among the control nodes of the k8s-cluster.</li> <li>Sets up new certs for the endpoint to be reachable</li> <li>Receives a <code>config</code> from Builder for <code>InstallVPN()</code></li> <li>Sets up ansible inventory, and installs the Wireguard full mesh VPN using a playbook</li> <li>Updates the <code>currentState</code> in a <code>config</code></li> <li>Receives a <code>config</code> from Builder for <code>InstallNodeRequirements()</code></li> <li>Sets up ansible inventory, and installs any prerequisities, as per individual nodes' requirements</li> <li>Updates the <code>currentState</code> in a <code>config</code></li> <li>Receives a <code>config</code> from Builder for <code>SetUpLoadbalancers()</code></li> <li>Sets up the ansible inventory, and installs nginx load balancers</li> <li>Creates and verifies the DNS configuration for the load balancers</li> </ul>"},{"location":"claudie-workflow/claudie-workflow/#kube-eleven","title":"Kube-eleven","text":"<p>Kube-eleven uses KubeOne to set up kubernetes clusters. After cluster creation, it assures the cluster stays healthy and keeps running smoothly.</p>"},{"location":"claudie-workflow/claudie-workflow/#api_5","title":"API","text":"<pre><code>  // BuildCluster builds the kubernetes clusters specified in the provided config.\nrpc BuildCluster(BuildClusterRequest) returns (BuildClusterResponse);\n</code></pre>"},{"location":"claudie-workflow/claudie-workflow/#flow_5","title":"Flow","text":"<ul> <li>Receives a <code>config</code> object from Builder</li> <li>Generates KubeOne manifest based on the <code>desiredState</code></li> <li>Uses KubeOne to provision a kubernetes cluster</li> <li>Updates the <code>currentState</code> in the <code>config</code></li> </ul>"},{"location":"claudie-workflow/claudie-workflow/#kuber","title":"Kuber","text":"<p>Kuber manipulates the cluster resources using <code>kubectl</code>.</p>"},{"location":"claudie-workflow/claudie-workflow/#api_6","title":"API","text":"<pre><code>  // StoreClusterMetatada creates a secret, which holds the private key and a list of public IP addresses of the cluster supplied.\nrpc StoreClusterMetadata(StoreClusterMetadataRequest) returns (StoreClusterMetadataResponse);\n// DeleteClusterMetatada deletes the secret holding the private key and public IP addresses of the cluster supplied.\nrpc DeleteClusterMetadata(DeleteClusterMetadataRequest) returns (DeleteClusterMetadataResponse);\n// SetUpStorage installs Longhorn into the cluster.\nrpc SetUpStorage(SetUpStorageRequest) returns (SetUpStorageResponse); // StoreKubeconfig creates a secret, which holds the kubeconfig of a Claudie-created cluster.\nrpc StoreKubeconfig(StoreKubeconfigRequest) returns (StoreKubeconfigResponse);\n// DeleteKubeconfig removes the secret that holds the kubeconfig of a Claudie-created cluster.\nrpc DeleteKubeconfig(DeleteKubeconfigRequest) returns (DeleteKubeconfigResponse);\n// DeleteNodes deletes the specified nodes from a k8s cluster.\nrpc DeleteNodes(DeleteNodesRequest) returns (DeleteNodesResponse);\n</code></pre>"},{"location":"claudie-workflow/claudie-workflow/#flow_6","title":"Flow","text":"<ul> <li>Receives a <code>config</code> from Builder for <code>SetUpStorage()</code></li> <li>Applies the <code>longhorn</code> deployment</li> <li>Receives a <code>config</code> from Builder for <code>StoreKubeconfig()</code></li> <li>Creates a kubernetes secret that holds the kubeconfig of the Claudie-created cluster</li> <li>Upon infrastructure deletion request, Kuber deletes the kubeconfig secret of the cluster being deleted</li> </ul>"},{"location":"claudie-workflow/claudie-workflow/#frontend","title":"Frontend","text":"<p>Frontend is a layer between the user and Claudie. New manifests are added as secrets into the kubernetes cluster where <code>k8s-sidecar</code> saves them into Frontend's file system and notifies the Frontend service via a HTTP request that the new manifests are now available.</p>"},{"location":"claudie-workflow/claudie-workflow/#api_7","title":"API","text":"<pre><code>This service is a gRPC client, thus it does not provide any API\n</code></pre>"},{"location":"claudie-workflow/claudie-workflow/#flow_7","title":"Flow","text":"<ul> <li>User applies a new secret holding a manifest</li> <li><code>k8s-sidecar</code> detects it and saves it to Frontend's file system</li> <li><code>k8s-sidecar</code> notifies Frontend via a HTTP request that changes have been made</li> <li>Frontend detects the new manifest and saves it to the database</li> <li>Upon deletion of user-created secrets, Frontend initiates a deletion process of the manifest</li> </ul>"},{"location":"config/config/","title":"Config spec","text":"<p>Config is a datastructure, which holds all of the data for Claudie microservices. It is saved in the database and passed from service to service.</p>"},{"location":"config/config/#config","title":"Config","text":"<p>Config holds data for a single manifest.</p> Name Type Description id string Config id name string Config name manifest string Client defined manifest desiredState Project Desired state from the manifest currentState Project Current state of the infra msChecksum bytes Manifest state checksum dsChecksum bytes Desired state checksum csChecksum bytes Current state checksum builderTTL int32 Builder time to live counter schedulerTTL int32 Scheduler time to live counter errorMessage string Error message from error encountered during execution"},{"location":"config/config/#project","title":"Project","text":"<p>Project represents the desired state of the manifest and the current state of the manifest.   | Name                 | Type                        | Description                  |   | -------------------- | --------------------------- | ---------------------------- |   | Name                 | string                      | Name of the project          |   | Clusters             | []K8scluster | Slice of kubernetes clusters |   | LoadBalancerClusters | []LBcluster   | Slice of load balancers      |</p>"},{"location":"config/config/#k8scluster","title":"K8scluster","text":"<p>K8scluster represents a single kubernetes cluster specified in the manifest.   | Name        | Type                        | Description                    |   | ----------- | --------------------------- | ------------------------------ |   | ClusterInfo | ClusterInfo | General info about the cluster |   | Network     | string                      | Network range for the VPN      |   | Kubernetes  | string                      | Kubernetes version             |</p>"},{"location":"config/config/#lbcluster","title":"LBcluster","text":"<p>LBcluster represents a single load balancer cluster specified in the manifest.   | Name        | Type                        | Description                                                          |   | ----------- | --------------------------- | -------------------------------------------------------------------- |   | ClusterInfo | ClusterInfo | General info about the cluster                                       |   | Roles       | []Role             | Load balancer role                                                   |   | DNS         | Dns                 | DNS information                                                      |   | TargetedK8s | string                      | Kubernetes cluster name of cluster this load balancer is assigned to |</p>"},{"location":"config/config/#clusterinfo","title":"ClusterInfo","text":"<p>ClusterInfo holds general information about the clusters.   | Name        | Type                    | Description                                 |   | ----------- | ----------------------- | ------------------------------------------- |   | Name        | string                  | Name of the cluster                         |   | Hash        | string                  | Random hash of the cluster                  |   | Public_key  | string                  | Public ssh key for the nodes                |   | Private_key | string                  | Private ssh key for the nodes               |   | Nodepools   | []Nodepool | Slice of node pools this cluster is made of |</p>"},{"location":"config/config/#role","title":"Role","text":"<p>Role represents a single loadbalancer role from the manifest.   | Name       | Type                  | Description                             |   | ---------- | --------------------- | --------------------------------------- |   | Name       | string                | Name of the role                        |   | Protocol   | string                | Protocol that load balancer will use    |   | Port       | int32                 | Load balancer port                      |   | TargetPort | int32                 | Port that load balancer will forward to |   | Target     | Target     | Targeted nodes                          |   | RoleType   | RoleType | Type of the role                        |</p>"},{"location":"config/config/#dns","title":"DNS","text":"<p>DNS holds general information about the DNS records.   | Name     | Type                  | Description                          |   | -------- | --------------------- | ------------------------------------ |   | DnsZone  | string                | DNS zone for the DNS records         |   | Hostname | string                | User specified hostname              |   | Provider | Provider | Provider for the DNS records         |   | Endpoint | string                | The whole hostname of the DNS record |</p>"},{"location":"config/config/#nodepool","title":"NodePool","text":"<p>NodePool represents a single nodepool from the manifest.   | Name       | Type                  | Description                                             |   | ---------- | --------------------- | ------------------------------------------------------- |   | Name       | string                | Name of the node pool                                   |   | Region     | string                | Region of the nodes                                     |   | ServerType | string                | Machine type of the nodes                               |   | Image      | string                | OS image of the nodes                                   |   | DiskSize   | int32                 | Disk size of the nodes                                  |   | Zone       | string                | Zone for the nodes                                      |   | Count      | int32                 | Count of the nodes                                      |   | Nodes      | []Node       | Slice of Nodes                                          |   | Provider   | Provider | Provider of the nodepools                               |   | IsControl  | bool                  | Flag to differentiate between control and compute nodes |</p>"},{"location":"config/config/#node","title":"Node","text":"<p>Node represents a single node from the nodepool.   | Name     | Type                  | Description                       |   | -------- | --------------------- | --------------------------------- |   | Name     | string                | Name of the node                  |   | Private  | string                | Private IP of the node in the VPN |   | Public   | string                | Public IP of the node             |   | NodeType | NodeType | Type of the node                  |</p>"},{"location":"config/config/#provider","title":"Provider","text":"<p>Provider represents a single provider from the manifest.   | Name                | Type   | Description                                                        |   | ------------------- | ------ | ------------------------------------------------------------------ |   | SpecName            | string | Provider name                                                      |   | CloudProviderName   | string | Cloud provider name. e.g. <code>gcp</code>, <code>hetzner</code>, <code>oci</code>, etc.            |   | Credentials         | string | Secret Credentials of the provider          |   | GcpProject          | string | GCP project (only required when using GCP as DNS provider)         |   | OciUserOcid         | string | OCID of the user                                                   |   | OciTenancyOcid      | string | OCID of the tenancy                                                |   | OciFingerprint      | string | Fingerprint of the private key saved in <code>Credentials</code>              |   | OciCompartmentOcid  | string | OCID of the compartment                                            |   | AwsAccessKey        | string | AWS access key to the secret key saved in the <code>Credentials</code>        |   | AzureSubscriptionId | string | Azure ID of the subscription                                       |   | AzureTenantId       | string | Azure ID of the Tenant                                             |   | AzureClientId       | string | AzureID of the Client; the client secret is saved in <code>Credentials</code> |</p>"},{"location":"config/config/#secret-credentials","title":"Secret credentials","text":"<p>The list of information saved in the <code>Credentials</code> field for each provider.   | Provider | Input Manifest field                                         |   | -------- | ------------------------------------------------------------ |   | GCP      | <code>credentials</code>     |   | Hetzner  | <code>credentials</code> |   | AWS      | <code>secret_key</code>      |   | OCI      | <code>private_key</code>     |   | Azure    | <code>client_secret</code> |</p>"},{"location":"config/config/#nodetype","title":"NodeType","text":"<p>NodeType specifies the type of the node.   | Value       | Description                                |   | ----------- | ------------------------------------------ |   | Worker      | Worker node                                |   | Master      | Master node                                |   | ApiEndpoint | Master node, which is also an API endpoint |</p>"},{"location":"config/config/#target","title":"Target","text":"<p>Target specifies which nodes are targeted by the load balancer.   | Value           | Description          |   | --------------- | -------------------- |   | K8sAllNodes     | All nodes in cluster |   | K8sControlPlane | Only Master nodes    |   | K8sComputePlane | Only Compute nodes   |</p>"},{"location":"config/config/#roletype","title":"RoleType","text":"<p>RoleType specifies the type of the role.   | Value     | Description              |   | --------- | ------------------------ |   | ApiServer | API server load balancer |   | Ingress   | Ingress load balancer    |</p>"},{"location":"config/config/#clustertype","title":"ClusterType","text":"<p>ClusterType specifies the type of the cluster.   | Value | Description           |   | ----- | --------------------- |   | K8s   | Kubernetes cluster    |   | LB    | Load balancer cluster |</p>"},{"location":"contributing/contributing/","title":"Contributing","text":""},{"location":"contributing/contributing/#bug-reports","title":"Bug reports","text":"<p>When you encounter a bug, please create a new issue and use our bug template. Before you submit, please check: - ...that the issue you want to open is not a duplicate - ...that you submitted the logs/screenshots of any errors and a concise way to reproduce the issue - ...the input manifest you used :warning:be careful not to include your cloud credentials:warning:</p>"},{"location":"contributing/release/","title":"How to release a new version of Claudie","text":"<p>The release process of Claudie consists of a few manual steps and a few automated steps.</p>"},{"location":"contributing/release/#manual-steps","title":"Manual steps","text":"<p>Whoever is responsible for creating a new release has to:</p> <ol> <li>Write a new entry to a relevant Changelog document</li> <li>Add release notes to the Releases page</li> <li>Publish a release</li> </ol>"},{"location":"contributing/release/#automated-steps","title":"Automated steps","text":"<p>After a new release is published, a release pipeline runs, which will:</p> <ol> <li>Build new images tagged with the release tag</li> <li>Push them to the container registry where anyone can pull them</li> <li>Add Claudie manifest files to the release assets, with image tags referencing this release</li> </ol>"},{"location":"crud/crud/","title":"CRUD for Claudie","text":"<p>This document describes how the user manages/communicates with Claudie deployed in a Kubernetes cluster.</p> <p>Claudie has a component called Frontend, which functions like an entrypoint to Claudie. Frontend uses <code>k8s-sidecar</code>, which is configured to pull secrets with a label <code>claudie.io/input-manifest</code> and save them to Frontend's file system. Frontend then picks them up and applies them to Claudie.</p>"},{"location":"crud/crud/#create","title":"Create","text":"<p>In order to create (apply) a new input manifest, the user needs to create a new secret in the namespace where Claudie is deployed. This secret needs needs to have: - a label <code>claudie.io/input-manifest</code> - a unique field name   - IMPORTANT: If two secrets share the same data field name, the manifest saved by <code>k8s-sidecar</code> gets overwritten, which may in turn lead to (unwanted) deletion of infrastructure.</p>"},{"location":"crud/crud/#example","title":"Example:","text":"<p>If you define an input manifest called <code>claudie-manifest.yaml</code> (see the example here) and apply it by: 1. Creating the secret by running <pre><code>kubectl create secret generic input-manifest --from-file=input-manifest.yaml -n claudie\n</code></pre> 2. Labeling the secret with label <code>claudie.io/input-manifest</code> by running <pre><code>kubectl label secret input-manifest claudie.io/input-manifest=my-fancy-manifest -n claudie\n</code></pre></p>"},{"location":"crud/crud/#read","title":"Read","text":"<p>The user and Claudie both share a single \"source of truth\" for the input manifests - Kubernetes secrets. Created in the Claudie namespace, they are accessible by both the user and Claudie. This makes users store input manifests in an IaC manner and can easily be configured for GitOps synchronization (i.e. via FluxCD).</p>"},{"location":"crud/crud/#update","title":"Update","text":"<p>When you want to update the input manifest, you can edit/reapply the secret with the updated input manifest inside of it (the secret name and the data field name will stay the same). <code>k8s-sidecar</code> notices the change in the secret data and subsequently updates the file inside Frontend's file system. Frontend then applies it to Claudie and the update of the defined infrastructure is underway.</p>"},{"location":"crud/crud/#delete","title":"Delete","text":"<p>If you wish to destroy your cluster along with the infrastructure, you can remove the cluster definition block from the input-manifest and update the k8s secret accordingly. If you wish to delete all of the clusters defined in an input-manifest, you simply need to delete the k8s secret containing the manifest. Both events will trigger the deletion process. This process deletes the current infrastructure and it also deletes all data related to the particular input manifest.</p>"},{"location":"crud/crud/#outputs","title":"Outputs","text":"<p>Claudie outputs two secrets in the namespace where it is deployed, after a successful run of the (input) manifest: * kubeconfigs, * cluster metadata to your clusters.</p> <p>The names of the secrets are derived as follows: <code>&lt;cluster-name&gt;-&lt;cluster-hash&gt;-{kubeconfig,metadata}</code>. The secrets can be accessed by printing and <code>base64</code>-decoding them.</p> <p>Example of how to decode a secret: <pre><code>kubectl get secrets -n claudie &lt;cluster-name&gt;-&lt;cluster-hash&gt;-kubeconfig -o jsonpath='{.data.secretdata}' | base64 -d &gt; your_kubeconfig.yaml\n</code></pre></p>"},{"location":"input-manifest/example/","title":"Example yaml file","text":"example.yaml<pre><code>name: ExampleManifest\n\n# providers field is used for defining the providers\n# every supported provider has an example in this input manifest\nproviders:\nhetzner:\n- name: hetzner-1\n#examaple token\ncredentials: kslISA878a6etYAfXYcg5iYyrFGNlCxcICo060HVEygjFs21nske76ksjKko21lp\ngcp:\n- name: gcp-1\n# example credentials\ncredentials: |\n{\n\"type\": \"service_account\",\n\"project_id\": \"project-claudie\",\n\"private_key_id\": \"bskdlo875s9087394763eb84e407903lskdimp439\",\n\"private_key\": \"-----BEGIN PRIVATE KEY-----\\nSKLOosKJUSDANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhki\\n-----END PRIVATE KEY-----\\n\",\n\"client_email\": \"claudie@project-claudie-123456.iam.gserviceaccount.com\",\n\"client_id\": \"109876543211234567890\",\"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n\"token_uri\": \"https://oauth2.googleapis.com/token\",\n\"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n\"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/claudie%40claudie-project-123456.iam.gserviceaccount.com\"\n}\ngcp_project: project-id\noci:\n- name: oci-1\nprivate_key: |\n-----BEGIN RSA PRIVATE KEY-----\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/==\n-----END RSA PRIVATE KEY-----\nkey_fingerprint: ab:cd:3f:34:33:22:32:34:54:54:45:76:76:78:98:aa\ntenancy_ocid: ocid2.tenancy.oc2..aaaaaaaayrsfvlvxc34o060kfdygsds21nske76ksjkko21lpsdfsfsgbrtghs\nuser_ocid: ocid2.user.oc2..aaaaaaaaaanyrsfvlvxc34o060kfdygsds21nske76ksjkko21lpsdfsf\ncompartment_ocid: ocid2.compartment.oc2..aaaaaaaaa2rsfvlvxc34o060kfdygsds21nske76ksjkko21lpsdfsf\naws:\n- name: aws-1\naccess_key: SLDUTKSHFDMSJKDIALASSD\nsecret_key: iuhbOIJN+oin/olikDSadsnoiSVSDsacoinOUSHD\nazure:\n- name: azure-1\nclient_secret: Abcd~EFg~H6Ijkls~ABC15sEFGK54s78X~Olk9\nsubscription_id: 6a4dfsg7-sd4v-f4ad-dsva-ad4v616fd512\ntenant_id: 54cdafa5-sdvs-45ds-546s-df651sfdt614\nclient_id: 0255sc23-76we-87g6-964f-abc1def2gh3l\n\n# nodepools field is used for defining the nodepool spec\n# you can think of them as a blueprints, not actual nodepools that will be created\nnodePools:\n# dynamic nodepools are created by claudie\ndynamic:\n# control nodes\n- name: control-hetzner # name of the nodepool\nproviderSpec:\nname: hetzner-1\nregion: hel1\nzone: hel1-dc2\ncount: 3 # number of nodes in node pool\nserver_type: cpx11\nimage: ubuntu-22.04 # OS image\ndisk_size: 50 # size of the disk\n# compute nodes\n- name: compute-hetzner\nproviderSpec:\nname: hetzner-1\nregion: hel1\nzone: hel1-dc2\ncount: 2\nserver_type: cpx11\nimage: ubuntu-22.04\ndisk_size: 50\n- name: control-gcp\nproviderSpec:\nname: gcp-1\nregion: europe-west1\nzone: europe-west1-c\ncount: 3\nserver_type: e2-medium\nimage: ubuntu-os-cloud/ubuntu-2204-jammy-v20221206\ndisk_size: 50\n- name: compute-gcp\nproviderSpec:\nname: gcp-1\nregion: europe-west1\nzone: europe-west1-c\ncount: 2\nserver_type: e2-small\nimage: ubuntu-os-cloud/ubuntu-2204-jammy-v20221206\ndisk_size: 50\n- name: control-oci\nproviderSpec:\nname: oci-1\nregion: eu-milan-1\nzone: hsVQ:EU-MILAN-1-AD-1\ncount: 3\nserver_type: VM.Standard2.1\n# ubuntu minimal\n# https://docs.oracle.com/en-us/iaas/images/image/674d0b41-aee8-4c0b-bf99-9e100d90f241/\nimage: ocid1.image.oc1.eu-frankfurt-1.aaaaaaaavvsjwcjstxt4sb25na65yx6i34bzdy5oess3pkgwyfa4hxmzpqeq\ndisk_size: 50\n- name: compute-oci\nproviderSpec:\nname: oci-1\nregion: eu-milan-1\nzone: hsVQ:EU-MILAN-1-AD-1\ncount: 2\nserver_type: VM.Standard2.1\n# ubuntu minimal\n# https://docs.oracle.com/en-us/iaas/images/image/674d0b41-aee8-4c0b-bf99-9e100d90f241/\nimage: ocid1.image.oc1.eu-frankfurt-1.aaaaaaaavvsjwcjstxt4sb25na65yx6i34bzdy5oess3pkgwyfa4hxmzpqeq\ndisk_size: 50\n- name: control-aws\nproviderSpec:\nname: aws-1\nregion: eu-central-1\nzone: eu-central-1c\ncount: 2\nserver_type: t3.medium\n# https://cloud-images.ubuntu.com/query/focal/server/released.current.txt\nimage: ami-0965bd5ba4d59211c\ndisk_size: 50\n- name: compute-aws\nproviderSpec:\nname: aws-1\nregion: eu-central-1\nzone: eu-central-1c\ncount: 2\nserver_type: t3.medium\n# https://cloud-images.ubuntu.com/query/focal/server/released.current.txt\nimage: ami-0965bd5ba4d59211c\ndisk_size: 50\n- name: control-azure\nproviderSpec:\nname: azure-1\nregion: West Europe\nzone: 1\ncount: 2\nserver_type: Standard_B2s\nimage: Canonical:0001-com-ubuntu-minimal-jammy:minimal-22_04-lts:22.04.202212120\ndisk_size: 50\n- name: compute-azure\nproviderSpec:\nname: azure-1\nregion: West Europe\nzone: 1\ncount: 2\nserver_type: Standard_B2s\nimage: Canonical:0001-com-ubuntu-minimal-jammy:minimal-22_04-lts:22.04.202212120\ndisk_size: 50\n# loadbalancer nodes\n- name: loadbalancer-1\nprovider:\nproviderSpec:\nname: gcp-1\nregion: europe-west1\nzone: europe-west1-c\ncount: 2\nserver_type: e2-small\nimage: ubuntu-os-cloud/ubuntu-2004-focal-v20220610\ndisk_size: 50\n- name: loadbalancer-2\nproviderSpec:\nname: hetzner-1\nregion: hel1\nzone: hel1-dc2\ncount: 2\nserver_type: cpx11\nimage: ubuntu-20.04\ndisk_size: 50\n\n# kubernetes field is used to define the k8s clusters\n# here we define two clusters, dev and prod\nkubernetes:\nclusters:\n# dev cluster\n- name: dev-cluster\nversion: v1.22.0 # kubernetes version must be supported by kubeone\nnetwork: 192.168.2.0/24 # private network range\n# pools define what type of machines cluster will use\n# only pools metioned here will get created\npools:\ncontrol:\n- control-hetzner\ncompute:\n- compute-hetzner\n# prod cluster\n- name: prod-cluster\nversion: v1.22.0\nnetwork: 192.168.2.0/24\n# we can reuse same nodepool spec and claudie will create new nodes\npools:\ncontrol:\n- control-hetzner\n- control-gcp\n- control-oci\n- control-aws\n- control-azure\ncompute:\n- compute-hetzner\n- compute-gcp\n- compute-oci\n- compute-aws\n- compute-azure\n\n# loadbalancers field defines loadbalncers used for the k8s clusters\nloadBalancers:\n# roles define loadbalancer roles\nroles:\n# here, we define api server loadbalancer, but you can also define ingress loadbalancer\n- name: apiserver\nprotocol: tcp\n# incoming port\nport: 6443\n# forwarded port\ntarget_port: 6443\n# tageted nodes\n# can be k8sControlPlane, k8sComputePlane or k8sAllNodes\ntarget: k8sControlPlane\n# Here we define actual loadbalancer clusters\nclusters:\n# api server lb for dev cluster\n- name: apiserver-lb-dev\nroles:\n- apiserver\n# dns spec where claudie will create DNS records for LB\ndns:\ndns_zone: dns-zone\nprovider: gcp-1\n#hostname: #left empty means the Claudie will create random hash as a hostname\n# name of targeted k8s cluster\ntargeted-k8s: dev-cluster\n# nodepools for loadbalancer\npools:\n- loadbalancer-1\n- name: apiserver-lb-prod\nroles:\n- apiserver\ndns:\ndns_zone: dns-zone\nprovider: gcp-1\n# this hostname will be used for DNS records\n# keep in mind that DNS zone will be included\nhostname: my.fancy.url\ntargeted-k8s: prod-cluster\npools:\n- loadbalancer-2\n</code></pre>"},{"location":"input-manifest/input-manifest/","title":"Input manifest","text":""},{"location":"input-manifest/input-manifest/#manifest","title":"Manifest","text":"<p>Manifest is a definition of the user's infrastructure. It contains cloud provider specification, nodepool specification, Kubernetes and loadbalancer clusters. </p> <ul> <li><code>name</code></li> </ul> <p>Name of the manifest. Must be unique across all manifests of the Claudie instance.</p> <ul> <li><code>providers</code> Providers</li> </ul> <p>Defines all your cloud provider configuration that will be used while infrastructure provisioning.</p> <ul> <li><code>nodepools</code> Nodepools</li> </ul> <p>Describes nodepools used for either kubernetes clusters or loadbalancer cluster defined in this manifest.</p> <ul> <li><code>kubernetes</code> Kubernetes</li> </ul> <p>List of Kubernetes cluster this manifest will manage.</p> <ul> <li><code>loadBalancers</code> Loadbalancer</li> </ul> <p>List of loadbalancer clusters the Kubernetes clusters may use.</p>"},{"location":"input-manifest/input-manifest/#providers","title":"Providers","text":"<p>Contains configurations for supported cloud providers. At least one provider needs to be defined.</p> <ul> <li><code>gcp</code> GCP</li> </ul> <p>List of configuration options for Google Cloud. This field is optional.</p> <ul> <li><code>hetzner</code> Hetzner</li> </ul> <p>List of configuration options for Hetzner Cloud . This field is optional.</p> <ul> <li><code>oci</code> OCI</li> </ul> <p>List of configuration options for Oracle Cloud Infrastructure. This field is optional.</p> <ul> <li><code>aws</code> AWS</li> </ul> <p>List of configuration options for Amazon Web Services. This field is optional.</p> <ul> <li><code>azure</code> Azure</li> </ul> <p>List of configuration options for Azure. This field is optional.</p> <p>Support for more cloud providers is in the roadmap. </p>"},{"location":"input-manifest/input-manifest/#gcp","title":"GCP","text":"<p>Collection of data defining GCP cloud provider configuration. </p> <p>To find out how to configure GCP provider and service account, follow the instructions here.</p> <ul> <li><code>name</code></li> </ul> <p>Name of the provider. Used as a reference further in the input manifest. Should be unique for each provider spec across all the cloud providers.</p> <ul> <li><code>credentials</code></li> </ul> <p>Credentials for the provider. Stringified JSON service account key.</p> <ul> <li><code>gcp_project</code></li> </ul> <p>project id of an already existing GCP project.</p>"},{"location":"input-manifest/input-manifest/#hetzner","title":"Hetzner","text":"<p>Collection of data defining Hetzner cloud provider configuration. </p> <p>To find out how to configure Hetzner provider and service account, follow the instructions here.</p> <ul> <li><code>name</code></li> </ul> <p>Name of the provider spec. Used as a reference further in the input manifest. Should be unique for each provider spec across all the cloud providers.</p> <ul> <li><code>credentials</code></li> </ul> <p>Credentials for the provider (API token).</p>"},{"location":"input-manifest/input-manifest/#oci","title":"OCI","text":"<p>Collection of data defining OCI cloud provider configuration. </p> <p>To find out how to configure OCI provider and service account, follow the instructions here.</p> <ul> <li><code>name</code></li> </ul> <p>Name of the provider spec. Used as a reference further in the input manifest. Should be unique for each provider spec across all the cloud providers.</p> <ul> <li><code>private_key</code></li> </ul> <p>Private key used to authenticate to the OCI.</p> <ul> <li><code>key_fingerprint</code></li> </ul> <p>Fingerprint of the user-supplied private key.</p> <ul> <li><code>tenancy_ocid</code></li> </ul> <p>OCID of the tenancy where <code>private_key</code> is added as an API key</p> <ul> <li><code>user_ocid</code></li> </ul> <p>OCID of the user in the supplied tenancy</p> <ul> <li><code>compartment_ocid</code></li> </ul> <p>OCID of the compartment where VMs/VCNs/... will be created</p>"},{"location":"input-manifest/input-manifest/#aws","title":"AWS","text":"<p>Collection of data defining AWS cloud provider configuration. </p> <p>To find out how to configure AWS provider and service account, follow the instructions here.</p> <ul> <li><code>name</code></li> </ul> <p>Name of the provider spec. Used as a reference further in the input manifest. Should be unique for each provider spec across all the cloud providers.</p> <ul> <li><code>access_key</code></li> </ul> <p>Access key ID for your AWS account.</p> <ul> <li><code>secret_key</code></li> </ul> <p>Secret key for the Access key specified above.</p>"},{"location":"input-manifest/input-manifest/#azure","title":"Azure","text":"<p>Collection of data defining Azure cloud provider configuration. </p> <p>To find out how to configure Azure provider and service account, follow the instructions here.</p> <ul> <li><code>name</code></li> </ul> <p>Name of the provider spec. Used as a reference further in the input manifest. Should be unique for each provider spec across all the cloud providers.</p> <ul> <li><code>subscription_id</code></li> </ul> <p>Subscription ID of your subscription in Azure.</p> <ul> <li><code>tenant_id</code></li> </ul> <p>Tenant ID of your tenancy in Azure.</p> <ul> <li><code>client_id</code></li> </ul> <p>Client ID of your client. The Claudie is design to use a service principal with appropriate permissions.</p> <ul> <li><code>client_secret</code></li> </ul> <p>Client secret generated for your client.</p>"},{"location":"input-manifest/input-manifest/#nodepools","title":"Nodepools","text":"<p>Collection of static and dynamic nodepool specification, to be referenced in the <code>kubernetes</code> or <code>loadBalancer</code> clusters.</p> <ul> <li><code>dynamic</code> Dynamic</li> </ul> <p>List of dynamically to-be-created nodepools of not yet existing machines, used for Kubernetes or loadbalancer clusters. </p> <p>These are only blueprints, and will only be created per reference in <code>kubernetes</code> or <code>loadBalancer</code> clusters. E.g. if the nodepool isn't used, it won't even be created. Or if the same nodepool is used in two different clusters, it will be created twice. In OOP analogy, a dynamic nodepool would be a class that would get instantiated <code>N &gt;= 0</code> times depending on which clusters reference it.</p> <ul> <li><code>static</code> [WORK IN PROGRESS]</li> </ul> <p>List of static nodepools of already existing machines, not created by of Claudie, used for Kubernetes or loadbalancer clusters. Typically, these would be on-premises machines.</p>"},{"location":"input-manifest/input-manifest/#dynamic","title":"Dynamic","text":"<p>Dynamic nodepools are defined for cloud provider machines that Claudie is expected to create.</p> <ul> <li><code>name</code></li> </ul> <p>Name of the nodepool. Each nodepool will have a random hash appended to the name, so the whole name will be of format <code>&lt;name&gt;-&lt;hash&gt;</code>.</p> <ul> <li><code>provideSpec</code> Provider spec</li> </ul> <p>Collection of provider data to be used while creating the nodepool.  </p> <ul> <li><code>count</code></li> </ul> <p>Number of the nodes in the nodepool.</p> <ul> <li><code>server_type</code></li> </ul> <p>Type of the machines in the nodepool.</p> <p>Currently, only AMD64 machines are supported.</p> <ul> <li><code>image</code></li> </ul> <p>OS image of the machine. </p> <p>Currently, only Ubuntu 22.04 AMD64 images are supported.</p> <ul> <li><code>disk_size</code></li> </ul> <p>Size of the disk on the nodes in the nodepool.</p>"},{"location":"input-manifest/input-manifest/#provider-spec","title":"Provider Spec","text":"<p>Provider spec is further specification build on top of the data from either GCP or Hetzner</p> <ul> <li><code>name</code></li> </ul> <p>Name of the provider specified in either GCP or Hetzner</p> <ul> <li><code>region</code></li> </ul> <p>Region of the nodepool. [NOTE: only used in GCP nodepools]</p> <ul> <li><code>zone</code></li> </ul> <p>Zone of the nodepool. Zone can be either GCP zone or Hetzner datacenter.</p>"},{"location":"input-manifest/input-manifest/#kubernetes","title":"Kubernetes","text":"<p>Defines Kubernetes clusters.</p> <ul> <li><code>clusters</code> Cluster-k8s</li> </ul> <p>List of Kubernetes clusters Claudie will create.</p>"},{"location":"input-manifest/input-manifest/#cluster-k8s","title":"Cluster-k8s","text":"<p>Collection of data used to define a Kubernetes cluster.</p> <ul> <li><code>name</code></li> </ul> <p>Name of the Kubernetes cluster. Each cluster will have a random hash appended to the name, so the whole name will be of format <code>&lt;name&gt;-&lt;hash&gt;</code>.</p> <ul> <li><code>version</code></li> </ul> <p>Kubernetes version of the cluster.</p> <p>Version should be defined in format <code>vX.Y</code>. In terms of supported versions of Kubernetes, Claudie follows <code>kubeone</code> releases and their supported versions. The current <code>kubeone</code> version used in Claudie is <code>1.5</code>. To see the list of supported versions, please refer to <code>kubeone</code> documentation.</p> <ul> <li><code>network</code></li> </ul> <p>Network range for the VPN of the cluster. The value should be defined in format <code>A.B.C.D/mask</code>.</p> <ul> <li><code>pools</code></li> </ul> <p>List of nodepool names this cluster will use. Remember that nodepools defined in nodepools are only \"blueprints\". The actual nodepool will be created once referenced here. </p>"},{"location":"input-manifest/input-manifest/#loadbalancer","title":"LoadBalancer","text":"<p>Defines loadbalancer clusters.</p> <ul> <li><code>roles</code> Role</li> </ul> <p>List of roles loadbalancers use to forward the traffic. Single role can be used in multiple loadbalancer clusters.</p> <ul> <li><code>clusters</code> Cluster-lb</li> </ul> <p>List of loadbalancer clusters used in the Kubernetes clusters defined under clusters.</p>"},{"location":"input-manifest/input-manifest/#role","title":"Role","text":"<p>Role defines a concrete loadbalancer configuration. Single loadbalancer can have multiple roles.</p> <ul> <li><code>name</code></li> </ul> <p>Name of the role. Used as a reference in clusters.</p> <ul> <li><code>protocol</code></li> </ul> <p>Protocol of the rule. Allowed values are:</p> Value Description <code>tcp</code> Role will use TCP protocol <code>udp</code> Role will use UDP protocol <ul> <li><code>port</code></li> </ul> <p>Port of the incoming traffic on the loadbalancer.</p> <ul> <li><code>target_port</code></li> </ul> <p>Port where loadbalancer forwards the traffic.</p> <ul> <li><code>target</code> </li> </ul> <p>Defines a target group of nodes. Allowed values are:</p> Value Description <code>k8sAllNodes</code> All nodes in the cluster <code>k8sControlNodes</code> Only control/master nodes in cluster <code>k8sComputeNodes</code> Only compute/worker nodes in cluster"},{"location":"input-manifest/input-manifest/#cluster-lb","title":"Cluster-lb","text":"<p>Collection of data used to define a loadbalancer cluster.</p> <ul> <li><code>name</code></li> </ul> <p>Name of the loadbalancer.</p> <ul> <li><code>roles</code></li> </ul> <p>List of roles the loadbalancer uses.</p> <ul> <li><code>dns</code> DNS</li> </ul> <p>Specification of the loadbalancer's DNS record.</p> <ul> <li><code>targeted-k8s</code></li> </ul> <p>Name of the Kubernetes cluster targetted by this loadbalancer.</p> <ul> <li><code>pools</code></li> </ul> <p>List of nodepool names this loadbalancer will use. Remember, that nodepools defined in nodepools are only \"blueprints\". The actual nodepool will be created once referenced here. </p>"},{"location":"input-manifest/input-manifest/#dns","title":"DNS","text":"<p>Collection of data Claudie uses to create a DNS record for the loadbalancer.</p> <ul> <li><code>dns_zone</code></li> </ul> <p>DNS zone inside of which the records will be created. For now, only a GCP DNS zone is accepted, thus making definition of the GCP provider necessary.</p> <ul> <li><code>provider</code></li> </ul> <p>Name of provider to be used for creating an A record entry in defined dns zone.</p> <ul> <li><code>hostname</code></li> </ul> <p>Custom hostname for your A record. If left empty, the hostname will be a random hash.</p>"},{"location":"input-manifest/providers/aws/","title":"AWS","text":"<p>In Claudie, the AWS cloud provider requires you to input the credentials as an <code>access_key</code> and a <code>secret_key</code> which will be linked to the IAM user in your account. It is important, that said IAM user will have sufficient policies attached. This will assure that Claudie will be able to create all resources for your infrastructure.</p>"},{"location":"input-manifest/providers/aws/#iam-policies-required-by-claudie","title":"IAM policies required by Claudie:","text":"<pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"ec2:*\"\n],\n\"Resource\": \"*\"\n}\n]\n}\n</code></pre>"},{"location":"input-manifest/providers/azure/","title":"Azure","text":"<p>In Claudie, Azure cloud provider requires you to input a few variables in order to function properly. These variables are -  <code>subscription_id</code> which is the ID to your subscription. Bear in mind that all resources you define needs to be supported by that subscription and will be charged there.</p> <ul> <li> <p><code>tenant_id</code> which is the ID of your tenant in the active directory.</p> </li> <li> <p><code>client_id</code> which is the ID for your service principal, under the tenancy you specified.</p> </li> <li> <p><code>client_secret</code> which is the secret, for specified service principal.</p> </li> </ul> <p>Furthermore, service principal has to have a certain role assigned to it. For VM and VPC management it is <code>Virtual Machine Contributor</code> and <code>Network Contributor</code> respectively; and for resource group creation and deletion,the permission are <pre><code>permissions {\nactions = [ \"Microsoft.Resources/subscriptions/resourceGroups/write\",\n\"Microsoft.Resources/subscriptions/resourceGroups/delete\",\n]\n}\n</code></pre></p>"},{"location":"input-manifest/providers/gcp/","title":"GCP","text":"<p>In Claudie, GCP cloud provider requires you to input <code>credentials</code> as well as specific project where the account resides. The credentials are in form of an account key in JSON. It is important, that account has sufficient IAM roles attached to it, so Claudie can create all resources for your infrastructure.</p> <p>Furthermore, your project should enable a few API, namely - <code>Compute Engine API</code> - <code>Cloud DNS API</code> </p> <p>when project will be used for Loadbalancers DNS</p>"},{"location":"input-manifest/providers/gcp/#dns-requirements","title":"DNS requirements","text":"<p>If your GCP provider will be used for DNS, you need to manually - set up dns zone - update domain name server</p> <p>since Claudie does not support their dynamic creation. </p>"},{"location":"input-manifest/providers/gcp/#iam-policies-required-by-claudie","title":"IAM policies required by Claudie:","text":"<ul> <li>for infrastructure creation</li> <li> <p><code>roles/roles/compute.admin</code></p> </li> <li> <p>for DNS</p> </li> <li><code>roles/dns.admin</code></li> </ul>"},{"location":"input-manifest/providers/hetzner/","title":"Hetzner","text":"<p>In Claudie, the Hetzner cloud provider requires you to input the credentials which  contain the API token for your Hetzner project. The API key should have <code>Read &amp; Write</code> permissions.</p> <p>To find out how you can create an API token please follow instructions here.</p>"},{"location":"input-manifest/providers/oci/","title":"OCI","text":"<p>In Claudie, OCI cloud provider requires you to input few variables which points to an account in your tenancy. To access the account, you need to provider tenancy OCID, user OCID, with user's API private key and fingerprint. Finally, you need to provide compartment OCID, which points to the compartment where resources for your infrastructure will be created.</p> <p>However, in order to create those resources, user needs to be in a group with the sufficient IAM policies.</p>"},{"location":"input-manifest/providers/oci/#iam-policies-required-by-claudie","title":"IAM policies required by Claudie:","text":"<pre><code>\"Allow group &lt;GROUP_NAME&gt; to manage instance-family in compartment &lt;COMPARTMENT_NAME&gt;\"\n\"Allow group &lt;GROUP_NAME&gt; to manage volume-family in compartment &lt;COMPARTMENT_NAME&gt;\"\n\"Allow group &lt;GROUP_NAME&gt; to manage virtual-network-family in tenancy\"\n</code></pre>"},{"location":"loadbalancing/loadbalancing-solution/","title":"Claudie load balancing solution","text":""},{"location":"loadbalancing/loadbalancing-solution/#loadbalancer","title":"Loadbalancer","text":"<p>To create a highly available kubernetes cluster, Claudie creates load balancers for the <code>kubeAPI</code> server. These load balancers use Nginx to load balance the traffic among the cluster nodes. Claudie also supports definition of custom load balancers for the applications running inside the cluster.</p>"},{"location":"loadbalancing/loadbalancing-solution/#concept","title":"Concept","text":"<ul> <li>The load balancer machines will join the Wireguard private network of Claudie clusters relevant to it</li> <li> <p>This is necessary so that the LB machines can send traffic to the cluster machines over the <code>wireguard VPN</code></p> </li> <li> <p>DNS A records will be created and managed by Claudie on 1 or more cloud providers</p> </li> <li> <p>There will be a DNS A record for the public IP of each LB machine that is currently passing the health checks</p> </li> <li> <p>The LB machines will run an <code>Nginx</code> to carry out the actual load balancing.</p> </li> <li>There will be a DNS A record for the public IP of each LB machine that is currently passing the health checks</li> <li> <p>Therefore, there will be actually 2 layers of load balancing </p> <ol> <li>DNS-based load balancing to determine the LB machine to be used </li> <li>Software load balancing on the chosen LB machine. </li> </ol> </li> <li> <p>Claudie will dynamically manage the LB configuration, e.g. if some cluster node is removed, the LB configuration changes or DNS configuration changes (hostname change)</p> </li> <li> <p>The load balancing will be on L4 layer, TCP/UDP, partially configurable by the Claudie input manifest</p> </li> </ul>"},{"location":"loadbalancing/loadbalancing-solution/#example-diagram","title":"Example diagram","text":""},{"location":"loadbalancing/loadbalancing-solution/#definitions","title":"Definitions","text":""},{"location":"loadbalancing/loadbalancing-solution/#role","title":"Role","text":"<p>Claudie uses the concept of roles while configuring the load balancers from the input manifest. Each role represents a loadbalancer configuration for a particular use. Roles are then assigned to the load balancer cluster. A single load balancer cluster can have multiple roles assigned.</p>"},{"location":"loadbalancing/loadbalancing-solution/#targeted-kubernetes-cluster","title":"Targeted kubernetes cluster","text":"<p>Load balancer gets assigned to a kubernetes cluster with the field <code>targeted-k8s</code>. This field is using the <code>name</code> of the kubernetes cluster as a value. Currently, a single load balancer can only be assigned to a single kubernetes cluster.</p> <p>Among multiple load balancers targeting the same kubernetes cluster only one of them can have the API server role (i.e. the role with target port 6443) attached to it.</p>"},{"location":"loadbalancing/loadbalancing-solution/#dns","title":"DNS","text":"<p>Claudie creates and manages the DNS for the load balancer. If the user adds a load balancer into their infrastructure via Claudie, Claudie creates a DNS A record with the public IP of the load balancer machines behind it. When the load balancer configuration changes in any way, that is a node is added/removed, the hostname or the target changes, the DNS record is reconfigured by Claudie on the fly. This rids the user of the need to manage DNS.</p>"},{"location":"loadbalancing/loadbalancing-solution/#nodepools","title":"Nodepools","text":"<p>Loadbalancers are build from user defined nodepools in <code>pools</code> field, similar to how kubernetes clusters are defined. These nodepools allow the user to change/scale the load balancers according to their needs without any fuss. See the nodepool definition for more information.</p>"},{"location":"loadbalancing/loadbalancing-solution/#an-example-of-load-balancer-definition","title":"An example of load balancer definition","text":"<p>See an example load balancer definition in our reference example input manifest.</p>"},{"location":"roadmap/roadmap/","title":"Roadmap for Claudie","text":"<ul> <li>Support for more cloud providers</li> <li> OCI</li> <li> AWS</li> <li> Azure</li> <li>Periodic health checks &amp; sync loops for the managed clusters</li> <li>Hybrid-cloud support (on-premises)</li> <li>arm64 support for the nodepools</li> <li>Autoscaler</li> </ul>"},{"location":"storage/storage-solution/","title":"Claudie storage solution","text":""},{"location":"storage/storage-solution/#concept","title":"Concept","text":"<p>Running stateful workloads is a complex task, even more so when considering the {multi,hybrid}-cloud environment. Claudie therefore needs to be able to accommodate stateful workloads, regardless of the underlying infrastructure providers.</p> <p>Claudie orchestrates storage on the kubernetes cluster nodes by creating one storage cluster across multiple providers. This storage cluster has a series of <code>zones</code>, one for each cloud provider. Each <code>zone</code> then stores its own persistent volume data.</p>"},{"location":"storage/storage-solution/#longhorn","title":"Longhorn","text":"<p>A Claudie-created cluster comes with the <code>longhorn</code> deployment preinstalled and ready to be used. By default, only worker nodes are used to store data.</p> <p>Longhorn installed in the cluster is set up in such a way that it provides one default <code>StorageClass</code> called <code>longhorn</code>, which - if used - creates a volume that is then replicated across random nodes in the cluster. </p> <p>Besides the default storage class, Claudie can also create custom storage classes, which force persistent volumes to be created on a specific nodes based on the provider they have. In other words, you can use a specific provider to provision nodes for your storage needs, while using another provider for computing tasks.</p>"},{"location":"storage/storage-solution/#example","title":"Example","text":"<p>To follow along, have a look at the reference example input manifest file.</p> <p>When Claudie applies this input manifest, the following storage classes are installed: - <code>longhorn</code> - the default storage class, which stores data on random nodes - <code>longhorn-&lt;provider&gt;-zone</code> - storage class, which stores data only on nodes of the specified providier (see the list of supported providers)</p> <p>For more information on how Longorn works you can check out Longhorn's official documentation.</p>"},{"location":"use-cases/use-cases/","title":"Use-cases and customers","text":"<p>We foresee the following use-cases of the Claudie platform</p>"},{"location":"use-cases/use-cases/#1-cloud-bursting","title":"1. Cloud-bursting","text":"<p>A company uses advanced cloud features in one of the hyper-scale providers (e.g. serverless Lambda and API Gateway functionality in AWS). They run a machine-learning application that they need to train for a pattern on a dataset. The learning phase requires significant compute resources. Claudie allows to extend the cluster in AWS (needed in order to access the AWS functionality) to Hetzner for saving the infrastructure costs of the machine-learning case.</p> <p>Typical client profiles: - startups - in need of significant computing power already in their early stages (e.g. AI/ML workloads)</p>"},{"location":"use-cases/use-cases/#2-cost-saving","title":"2. Cost-saving","text":"<p>A company would like to utilize their on-premise or leased resources that they already invested into, but would like to: 1. extend the capacity 2. access managed features of a hyper-scale provider (AWS, GCP, ...) 3. get the workload physically closer to a client (e. g. to South America)</p> <p>Typical client profile: - medium-size business - possibly already familiar with containerized workload</p>"},{"location":"use-cases/use-cases/#3-smart-layer-as-a-service-on-top-of-simple-cloud-providers","title":"3. Smart-layer-as-a-Service on top of simple cloud-providers","text":"<p>An existing customer of medium-size provider (e.g. Exoscale) would like to utilize features that are typical for hyper-scale providers. Their current provider does neither offer nor plan to offer such an advanced functionality.</p> <p>Typical client profile: - established business - need to access advanced managed features to innovate faster</p>"},{"location":"use-cases/use-cases/#4-service-interconnect","title":"4. Service interconnect","text":"<p>A company would like to access on-premise-hosted services and cloud-managed services from within the same cluster. For on-premise services the on-premise cluster node would egress the traffic. The cloud-hosted cluster nodes would deal with the egress traffic to the cloud-managed services.</p> <p>Typical client profile: - medium-size/established business - already contains on-premise workloads - has the need to take the advantage of managed cloud infra (from cost, agility, or capacity reasons)</p>"}]}