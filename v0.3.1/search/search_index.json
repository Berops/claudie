{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Docs Overview","text":"<p>In case you are reading our documentation in our GitHub repository, you might want to read the prettier version of it on docs.claudie.io</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>The \"Getting Started\" section is where you'll learn how to begin using Claudie. We'll guide you through the initial steps and show you how to set things up, so you can start using the software right away. </p> <p>You'll also find helpful information on how to customize Claudie to suit your needs, including specifications for the settings you can adjust, and examples of how to use configuration files to get started.</p> <p>By following the steps in this section, you'll have everything you need to start using Claudie with confidence!</p>"},{"location":"#how-claudie-works","title":"How Claudie works","text":"<p>In this section, we'll show you how Claudie works and guide you through our workflow. We'll explain how we store and manage data, balance the workload across different parts of the system, and automatically adjust resources to handle changes in demand.</p> <p>By following our explanations, you'll gain a better understanding of how Claudie operates and be better equipped to use it effectively.</p>"},{"location":"#claudie-use-cases","title":"Claudie Use Cases","text":"<p>The \"Claudie Use Cases\" section includes examples of different ways you can use Claudie to solve various problems. We've included these examples to help you understand the full range of capabilities Claudie offers and to show you how it can be applied in different scenarios. </p> <p>By exploring these use cases, you'll get a better sense of how Claudie can be a valuable tool for your work.</p>"},{"location":"#roadmap-for-claudie","title":"Roadmap for Claudie","text":"<p>In this section, you'll find a roadmap for Claudie that outlines the features we've already added and those we plan to add in the future.</p> <p>By checking out the roadmap, you'll be able to stay informed about the latest updates and see how Claudie is evolving to meet the needs of its users.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>In this section, we've gathered all the information you'll need if you want to help contribute to the Claudie project or release a new version of the software. </p> <p>By checking out this section, you'll get a better sense of what's involved in contributing and how you can be part of making Claudie even better.</p>"},{"location":"#changelog","title":"Changelog","text":"<p>The \"changelog\" section is where you can find information about all the changes, updates, and issues related to each version of Claudie. </p>"},{"location":"CHANGELOG/changelog-0.1.x/","title":"Claudie <code>v0.1</code>","text":"<p>The first official release of Claudie</p>"},{"location":"CHANGELOG/changelog-0.1.x/#deployment","title":"Deployment","text":"<p>To deploy the Claudie <code>v0.1.X</code>, please:</p> <ol> <li> <p>Download the archive and checksums from the release page</p> </li> <li> <p>Verify the archive with the <code>sha256</code> (optional)</p> <pre><code>sha256sum -c --ignore-missing checksums.txt\n</code></pre> <p>If valid, output is, depending on the archive downloaded</p> <pre><code>claudie.tar.gz: OK\n</code></pre> <p>or</p> <pre><code>claudie.zip: OK\n</code></pre> <p>or both.</p> </li> <li> <p>Lastly, unpack the archive and deploy using <code>kubectl</code></p> <p>We strongly recommend changing the default credentials for MongoDB, MinIO and DynamoDB before you deploy it. To do this, change contents of the files in <code>mongo/secrets</code>, <code>minio/secrets</code> and <code>dynamo/secrets</code> respectively.</p> <pre><code>kubectl apply -k .\n</code></pre> </li> </ol>"},{"location":"CHANGELOG/changelog-0.1.x/#v013","title":"v0.1.3","text":""},{"location":"CHANGELOG/changelog-0.1.x/#features","title":"Features","text":"<ul> <li>Change the workflow of the Claudie, to build the infrastructure on per cluster basis #584</li> <li>Add labels on Claudie created resources #579</li> </ul>"},{"location":"CHANGELOG/changelog-0.1.x/#bugfixes","title":"Bugfixes","text":"<p>No bugfixes since the last release.</p>"},{"location":"CHANGELOG/changelog-0.1.x/#known-issues","title":"Known issues","text":"<ul> <li><code>k8s-sidecar</code> sometimes misses deletion of the input manifest secret #588</li> <li>Deleting nodes in Builder is not idempotent #587</li> </ul>"},{"location":"CHANGELOG/changelog-0.1.x/#v012","title":"v0.1.2","text":""},{"location":"CHANGELOG/changelog-0.1.x/#features_1","title":"Features","text":"<ul> <li>Update to Go <code>v1.20</code> #559</li> <li>The VPN now respects netmask from defined CIDR #571</li> <li>Connections attempt are more readable #570</li> </ul>"},{"location":"CHANGELOG/changelog-0.1.x/#bugfixes_1","title":"Bugfixes","text":"<ul> <li>Wireguard IP now persists across reboots #557</li> <li>Deletion of the infrastructure before any outputs were created does not end with error #569</li> <li>Replace the <code>azurerm_virtual_machine</code> to the <code>azurerm_linux_virtual_machine</code> #573</li> </ul>"},{"location":"CHANGELOG/changelog-0.1.x/#known-issues_1","title":"Known issues","text":"<ul> <li>Longhorn replicas are  not properly managed, which might cause issues when deleting nodes #564</li> <li>Naming scheme in input manifest is not uniform #563</li> </ul>"},{"location":"CHANGELOG/changelog-0.1.x/#v011","title":"v0.1.1","text":""},{"location":"CHANGELOG/changelog-0.1.x/#features_2","title":"Features","text":"<ul> <li>Support DNS zone for Cloudflare, AWS, Azure, HetznerDNS, OCI #530</li> <li>Add default node labels #543</li> </ul>"},{"location":"CHANGELOG/changelog-0.1.x/#bugfixes_2","title":"Bugfixes","text":"<ul> <li>Logs in all services have been modified to not output sensitive information. #535</li> <li>Correctly update desiredState after workflow for a given manifest completes. #536</li> </ul>"},{"location":"CHANGELOG/changelog-0.1.x/#known-issues_2","title":"Known issues","text":"<ul> <li>Wireguard interface <code>wg0</code> is missing ip address after reboot. Will be fixed in next release #557</li> </ul>"},{"location":"CHANGELOG/changelog-0.1.x/#v010","title":"v0.1.0","text":""},{"location":"CHANGELOG/changelog-0.1.x/#features_3","title":"Features","text":"<ul> <li>Multi-cloud kubernetes cluster management</li> <li>Multi-cloud loadbalancer management</li> <li>Fast scale-up/scale-down of defined infrastructure</li> <li>Persistent storage via Longhorn</li> <li>Support for AWS, Azure, GCP, OCI and Hetzner</li> <li>GCP DNS zone support</li> <li>Claudie deployment on <code>amd64</code> and <code>arm64</code> clusters</li> </ul>"},{"location":"CHANGELOG/changelog-0.1.x/#bugfixes_3","title":"Bugfixes","text":"<ul> <li>As this is first release there are no bugfixes</li> </ul>"},{"location":"CHANGELOG/changelog-0.1.x/#known-issues_3","title":"Known issues","text":"<ul> <li><code>iptables</code> reset after reboot and block all traffic on <code>OCI</code> node #466</li> <li>Occasional connection issues between Claudie created clusters and Claudie on Hetzner and GCP #276</li> <li>Unable to easily recover after error #528</li> </ul>"},{"location":"CHANGELOG/changelog-0.2.x/","title":"Claudie <code>v0.2</code>","text":"<p>Due to a breaking change in the input manifest schema, the <code>v0.2.x</code> will not be backwards compatible with <code>v0.1.x</code>.</p>"},{"location":"CHANGELOG/changelog-0.2.x/#deployment","title":"Deployment","text":"<p>To deploy the Claudie <code>v0.2.X</code>, please:</p> <ol> <li> <p>Download the archive and checksums from the release page</p> </li> <li> <p>Verify the archive with the <code>sha256</code> (optional)</p> <pre><code>sha256sum -c --ignore-missing checksums.txt\n</code></pre> <p>If valid, output is, depending on the archive downloaded</p> <pre><code>claudie.tar.gz: OK\n</code></pre> <p>or</p> <pre><code>claudie.zip: OK\n</code></pre> <p>or both.</p> </li> <li> <p>Lastly, unpack the archive and deploy using <code>kubectl</code></p> <p>We strongly recommend changing the default credentials for MongoDB, MinIO and DynamoDB before you deploy it. To do this, change contents of the files in <code>mongo/secrets</code>, <code>minio/secrets</code> and <code>dynamo/secrets</code> respectively.</p> <pre><code>kubectl apply -k .\n</code></pre> </li> </ol>"},{"location":"CHANGELOG/changelog-0.2.x/#v020","title":"v0.2.0","text":""},{"location":"CHANGELOG/changelog-0.2.x/#features","title":"Features","text":"<ul> <li>Unify the naming schema in the input manifest #601</li> <li>Deploy MinIO in multi-replica fashion #589</li> </ul>"},{"location":"CHANGELOG/changelog-0.2.x/#bugfixes","title":"Bugfixes","text":"<p>No bugfixes since the last release.</p>"},{"location":"CHANGELOG/changelog-0.2.x/#known-issues","title":"Known issues","text":"<ul> <li>Workflow fails to build when a user makes multiple changes of the input manifest, regarding the API endpoint #606</li> <li>Longhorn pod longhorn-admission-webhook stuck in Init state #598</li> <li>Deletion of config fails if builder crashes after deleting nodes #588</li> </ul>"},{"location":"CHANGELOG/changelog-0.2.x/#v021","title":"v0.2.1","text":""},{"location":"CHANGELOG/changelog-0.2.x/#features_1","title":"Features","text":"<ul> <li>Improve management of Longhorn volume replicas #648</li> <li>Improve logging on all services #657</li> </ul>"},{"location":"CHANGELOG/changelog-0.2.x/#bugfixes_1","title":"Bugfixes","text":"<ul> <li>Fix unnecessary restarts in Wireguard playbook #658</li> </ul>"},{"location":"CHANGELOG/changelog-0.2.x/#known-issues_1","title":"Known issues","text":"<ul> <li>Certain change in nodepool configuration forces replacement of VMs #647</li> </ul>"},{"location":"CHANGELOG/changelog-0.2.x/#v022","title":"v0.2.2","text":""},{"location":"CHANGELOG/changelog-0.2.x/#features_2","title":"Features","text":"<ul> <li>Cluster Autoscaler integration #644</li> <li>Drop using grpc-health-probe in favour of Kubernetes native grpc health probes. #691</li> <li>Centralized information about the current workflow state of a cluster in the frontend service #605</li> </ul>"},{"location":"CHANGELOG/changelog-0.2.x/#bugfixes_2","title":"Bugfixes","text":"<ul> <li>Certain change in nodepool configuration forces replacement of VMs #647</li> </ul>"},{"location":"CHANGELOG/changelog-0.3.x/","title":"Claudie <code>v0.3</code>","text":"<p>Due to a breaking change in the input manifest schema, the <code>v0.3.x</code> will not be backwards compatible with <code>v0.2.x</code></p>"},{"location":"CHANGELOG/changelog-0.3.x/#deployment","title":"Deployment","text":"<p>To deploy the Claudie <code>v0.3.X</code>, please:</p> <ol> <li> <p>Download the archive and checksums from the release page</p> </li> <li> <p>Verify the archive with the <code>sha256</code> (optional)</p> <pre><code>sha256sum -c --ignore-missing checksums.txt\n</code></pre> <p>If valid, output is, depending on the archive downloaded</p> <pre><code>claudie.tar.gz: OK\n</code></pre> <p>or</p> <pre><code>claudie.zip: OK\n</code></pre> <p>or both.</p> </li> <li> <p>Lastly, unpack the archive and deploy using <code>kubectl</code></p> <p>We strongly recommend changing the default credentials for MongoDB, MinIO and DynamoDB before you deploy it. To do this, change contents of the files in <code>mongo/secrets</code>, <code>minio/secrets</code> and <code>dynamo/secrets</code> respectively.</p> <pre><code>kubectl apply -k .\n</code></pre> </li> </ol>"},{"location":"CHANGELOG/changelog-0.3.x/#v030","title":"v0.3.0","text":""},{"location":"CHANGELOG/changelog-0.3.x/#features","title":"Features","text":"<ul> <li>Use separate storage disk for longhorn #689</li> <li>Apply proper kubernetes labels to Claudie resources #714</li> <li>Implement clean architecture for the Frontend #701</li> </ul>"},{"location":"CHANGELOG/changelog-0.3.x/#bugfixes","title":"Bugfixes","text":"<ul> <li>Fix logging issues in Frontend #713</li> </ul>"},{"location":"CHANGELOG/changelog-0.3.x/#known-issues","title":"Known issues","text":"<ul> <li>Infrastructure might not get deleted if workflow encounters and error #712</li> <li>Certain cluster manipulation can result in workflow failing to build the clusters #606</li> </ul>"},{"location":"CHANGELOG/changelog-0.3.x/#v031","title":"v0.3.1","text":""},{"location":"CHANGELOG/changelog-0.3.x/#features_1","title":"Features","text":"<ul> <li>Rework logs in all microservices to enable easier filtering #742</li> <li>Improve longhorn volume replication management #782</li> <li>Various improvements in cluster manipulation #728</li> <li>Removal of <code>k8s-sidecar</code> from Frontend #792</li> </ul>"},{"location":"CHANGELOG/changelog-0.3.x/#bugfixes_1","title":"Bugfixes","text":"<ul> <li>Fixed bug when infrastructure was not deleted if workflow encountered an error #773</li> <li>Fixed error when deletion of nodes from cluster failed #728</li> <li>Fixed bug when frontend triggered deletion of incorrect manifest #744</li> </ul>"},{"location":"CHANGELOG/changelog-0.3.x/#known-issues_1","title":"Known issues","text":"<ul> <li>Subnet CIDR is not carried over from temporary state in Builder #790</li> <li>Longhorn occasionally does not detach volume from node which was deleted #784</li> </ul>"},{"location":"autoscaling/autoscaling/","title":"Autoscaling in Claudie","text":"<p>Claudie supports autoscaling by installing Cluster Autoscaler for Claudie-made clusters, with a custom implementation of <code>external gRPC cloud provider</code>, in Claudie context called <code>autoscaler-adapter</code>. This, together with Cluster Autoscaler is automatically managed by Claudie, for any clusters, which have at least one node pool defined with <code>autoscaler</code> field. Whats more, you can change the node pool specification freely from autoscaler configuration to static count or vice versa. Claudie will seamlessly configure Cluster Autoscaler, or even remove it when it is no longer needed.</p>"},{"location":"autoscaling/autoscaling/#what-triggers-a-scale-up","title":"What triggers a scale up","text":"<p>The scale up is triggered if there are pods in the cluster, which are unschedulable and</p> <ul> <li>could be scheduled, if any of the node pools with autoscaling enabled would accommodate them if they would grow in size</li> <li>the node pools, which could accommodate them, are not yet at maximum size</li> </ul> <p>However, if pods' resource requests are larger than any new node would offer, the scale up will not be triggered. The cluster is scanned every 10 seconds for these pods, to assure quick response to the cluster needs. For more information, please have a look at official Cluster Autoscaler documentation.</p>"},{"location":"autoscaling/autoscaling/#what-triggers-a-scale-down","title":"What triggers a scale down","text":"<p>The scale down is triggered, if all following conditions are met</p> <ul> <li>the sum of CPU and memory requests of all pods running on node considered for scale down is below 50% (Claudie by default excludes DaemonSet pods and Mirror pods)</li> <li>all pods running on the node (except those that run on all nodes by default, like manifest-run pods or pods created by DaemonSets) considered for scale down,  can be scheduled to other nodes</li> <li>the node considered for scale down does not have scale-down disabled annotation</li> </ul> <p>For more information, please have a look at official Cluster Autoscaler documentation.</p>"},{"location":"autoscaling/autoscaling/#architecture","title":"Architecture","text":"<p>As stated earlier, Claudie deploys Cluster Autoscaler and Autoscaler Adapter for every Claudie-made cluster which enables it. These components are deployed within the same cluster as Claudie.</p> <p></p>"},{"location":"autoscaling/autoscaling/#considerations","title":"Considerations","text":"<p>As Claudie just extends Cluster Autoscaler, it is important that you follow their best practices. Furthermore, as number of nodes in autoscaled node pools can be volatile, you should carefully plan out how you will use the storage on such node pools. Longhorn support of Cluster Autoscaler is still in experimental phase (longhorn documentation).</p>"},{"location":"claudie-workflow/claudie-workflow/","title":"Claudie","text":""},{"location":"claudie-workflow/claudie-workflow/#a-single-platform-for-multiple-clouds","title":"A single platform for multiple clouds","text":""},{"location":"claudie-workflow/claudie-workflow/#microservices","title":"Microservices","text":"<ul> <li>Context-box</li> <li>Scheduler</li> <li>Builder</li> <li>Terraformer</li> <li>Ansibler</li> <li>Kube-eleven</li> <li>Kuber</li> <li>Frontend</li> </ul>"},{"location":"claudie-workflow/claudie-workflow/#data-stores","title":"Data stores","text":"<ul> <li>MongoDB</li> <li>Minio</li> <li>DynamoDB</li> </ul>"},{"location":"claudie-workflow/claudie-workflow/#tools-used","title":"Tools used","text":"<ul> <li>Terraform</li> <li>Ansible</li> <li>KubeOne</li> <li>Longhorn</li> <li>Nginx</li> <li>Calico</li> <li>gRPC</li> </ul>"},{"location":"claudie-workflow/claudie-workflow/#context-box","title":"Context-box","text":"<p>Context box is Claudie's \"control unit\". It holds pending configs, which need to be processed, periodically checks for new/changed configs and receives new configs from <code>frontend</code>.</p>"},{"location":"claudie-workflow/claudie-workflow/#api","title":"API","text":"<pre><code>  // SaveConfigFrontEnd saves the config parsed by Frontend.\nrpc SaveConfigFrontEnd(SaveConfigRequest) returns (SaveConfigResponse);\n// SaveConfigScheduler saves the config parsed by Scheduler.\nrpc SaveConfigScheduler(SaveConfigRequest) returns (SaveConfigResponse);\n// SaveConfigBuilder saves the config parsed by Builder.\nrpc SaveConfigBuilder(SaveConfigRequest) returns (SaveConfigResponse);\n// GetConfigFromDB gets a single config from the database.\nrpc GetConfigFromDB(GetConfigFromDBRequest) returns (GetConfigFromDBResponse);\n// GetConfigScheduler gets a config from Scheduler's queue of pending configs.\nrpc GetConfigScheduler(GetConfigRequest) returns (GetConfigResponse);\n// GetConfigBuilder gets a config from Builder's queue of pending configs.\nrpc GetConfigBuilder(GetConfigRequest) returns (GetConfigResponse);\n// GetAllConfigs gets all configs from the database.\nrpc GetAllConfigs(GetAllConfigsRequest) returns (GetAllConfigsResponse);\n// DeleteConfig sets the manifest to null, effectively forcing the deletion of the infrastructure\n// defined by the manifest on the very next config (diff-) check.\nrpc DeleteConfig(DeleteConfigRequest) returns (DeleteConfigResponse);\n// DeleteConfigFromDB deletes the config from the database.\nrpc DeleteConfigFromDB(DeleteConfigRequest) returns (DeleteConfigResponse);\n// UpdateNodepool updates specific nodepool from the config. Used mainly for autoscaling.\nrpc UpdateNodepool(UpdateNodepoolRequest) returns (UpdateNodepoolResponse);\n</code></pre>"},{"location":"claudie-workflow/claudie-workflow/#flow","title":"Flow","text":"<ul> <li>Receives a <code>config</code> from Frontend, calculates its <code>msChecksum</code> and saves it to the database</li> <li>Periodically checks for <code>config</code> changes and pushes the <code>config</code> to the <code>schedulerQueue</code> if <code>msChecksum</code> != <code>dsChecksum</code></li> <li>Periodically checks for <code>config</code> changes and pushes the <code>config</code> to the <code>builderQueue</code> if <code>dsChecksum</code> != <code>csChecksum</code></li> <li>Receives a <code>config</code> with the <code>desiredState</code> from Scheduler and saves it to the database</li> <li>Receives a <code>config</code> with the <code>currentState</code> from Builder and saves it to the database</li> </ul>"},{"location":"claudie-workflow/claudie-workflow/#variables-used","title":"Variables used","text":"variable meaning <code>msChecksum</code> manifest checksum <code>dsChecksum</code> desired state checksum <code>csChecksum</code> current state checksum"},{"location":"claudie-workflow/claudie-workflow/#scheduler","title":"Scheduler","text":"<p>Scheduler brings the infrastructure to a desired the state based on the manifest contained in the config that is received from Context-box.</p> <p>Scheduler also monitors the health of current infrastructure and manages any operations based on actual health state (e.g. replacement of broken nodes, etc. [work in progress]).</p>"},{"location":"claudie-workflow/claudie-workflow/#api_1","title":"API","text":"<p>This service is a gRPC client, thus it does not provide any API</p>"},{"location":"claudie-workflow/claudie-workflow/#flow_1","title":"Flow","text":"<ul> <li>Periodically pulls <code>config</code> from Context-Box's <code>schedulerQueue</code></li> <li>Creates <code>desiredState</code> with <code>dsChecksum</code> based on the <code>config</code></li> <li>Sends the <code>config</code> file back to Context-box</li> </ul>"},{"location":"claudie-workflow/claudie-workflow/#builder","title":"Builder","text":"<p>Builder aligns the current state of the infrastructure with the desired state. It calls methods on <code>terraformer</code>, <code>ansibler</code>, <code>kube-eleven</code> and <code>kuber</code> in order to manage the infrastructure. It follows that Builder also takes care of deleting nodes from a kubernetes cluster by finding differences between <code>desiredState</code> and <code>currentState</code>.</p>"},{"location":"claudie-workflow/claudie-workflow/#api_2","title":"API","text":"<p>This service is a gRPC client, thus it does not provide any API</p>"},{"location":"claudie-workflow/claudie-workflow/#flow_2","title":"Flow","text":"<ul> <li>Periodically polls Context-Box's <code>builderQueue</code> for changes in <code>config</code>, pulls it when changed</li> <li>Calls Terraformer, Ansibler, Kube-eleven and Kuber</li> <li>Creates <code>currentState</code></li> <li>Sends updated <code>config</code> with the <code>currentState</code> to Context-box</li> </ul>"},{"location":"claudie-workflow/claudie-workflow/#terraformer","title":"Terraformer","text":"<p>Terraformer creates or destroys infrastructure (specified in the desired state) via Terraform calls.</p>"},{"location":"claudie-workflow/claudie-workflow/#api_3","title":"API","text":"<pre><code>  // BuildInfrastructure builds the infrastructure based on the provided desired state (includes addition/deletion of *stuff*).\nrpc BuildInfrastructure(BuildInfrastructureRequest) returns (BuildInfrastructureResponse);\n// DestroyInfrastructure destroys the infrastructure completely.\nrpc DestroyInfrastructure(DestroyInfrastructureRequest) returns (DestroyInfrastructureResponse);\n</code></pre>"},{"location":"claudie-workflow/claudie-workflow/#flow_3","title":"Flow","text":"<ul> <li>Receives a <code>config</code> from Builder</li> <li>Uses Terraform to create infrastructure based on the <code>desiredState</code></li> <li>Updates the <code>currentState</code> in the <code>config</code></li> <li>Upon receiving a deletion request, Terraformer destroys the infrastructure based on the current state</li> </ul>"},{"location":"claudie-workflow/claudie-workflow/#ansibler","title":"Ansibler","text":"<p>Ansibler uses Ansible to:</p> <ul> <li>set up Wireguard VPN between the nodes</li> <li>set up nginx load balancer</li> <li>install dependencies for nodes in a kubernetes cluster</li> </ul>"},{"location":"claudie-workflow/claudie-workflow/#api_4","title":"API","text":"<pre><code>  // InstallNodeRequirements installs any requirements there are on all of the nodes.\nrpc InstallNodeRequirements(InstallRequest) returns (InstallResponse);\n// InstallVPN sets up a VPN between the nodes in the k8s cluster and LB clusters.\nrpc InstallVPN(InstallRequest) returns (InstallResponse);\n// SetUpLoadbalancers sets up the load balancers together with the DNS and verifies their configuration.\nrpc SetUpLoadbalancers(SetUpLBRequest) returns (SetUpLBResponse);\n// TeardownLoadBalancers correctly destroys the load balancers attached to a k8s\n// cluster by choosing a new ApiServer endpoint.\nrpc TeardownLoadBalancers(TeardownLBRequest) returns (TeardownLBResponse);\n// UpdateAPIEndpoint handles changes of API endpoint between control nodes.\n// It will update the current stage based on the information from the desired state.\nrpc UpdateAPIEndpoint(UpdateAPIEndpointRequest) returns (UpdateAPIEndpointResponse);\n</code></pre>"},{"location":"claudie-workflow/claudie-workflow/#flow_4","title":"Flow","text":"<ul> <li>Receives a <code>configToDelete</code> from Builder for <code>TeardownLoadBalancers()</code></li> <li>Finds the new ApiEndpoint among the control nodes of the k8s-cluster.</li> <li>Sets up new certs for the endpoint to be reachable</li> <li>Receives a <code>config</code> from Builder for <code>InstallVPN()</code></li> <li>Sets up ansible inventory, and installs the Wireguard full mesh VPN using a playbook</li> <li>Updates the <code>currentState</code> in a <code>config</code></li> <li>Receives a <code>config</code> from Builder for <code>InstallNodeRequirements()</code></li> <li>Sets up ansible inventory, and installs any prerequisites, as per individual nodes' requirements</li> <li>Updates the <code>currentState</code> in a <code>config</code></li> <li>Receives a <code>config</code> from Builder for <code>SetUpLoadbalancers()</code></li> <li>Sets up the ansible inventory, and installs nginx load balancers</li> <li> <p>Creates and verifies the DNS configuration for the load balancers</p> </li> <li> <p><code>UpdateAPIEndpoint()</code> is called in specific use cases when there is change  in the api endpoint of a control plane.</p> </li> </ul>"},{"location":"claudie-workflow/claudie-workflow/#kube-eleven","title":"Kube-eleven","text":"<p>Kube-eleven uses KubeOne to set up kubernetes clusters. After cluster creation, it assures the cluster stays healthy and keeps running smoothly.</p>"},{"location":"claudie-workflow/claudie-workflow/#api_5","title":"API","text":"<pre><code>  // BuildCluster builds the kubernetes clusters specified in the provided config.\nrpc BuildCluster(BuildClusterRequest) returns (BuildClusterResponse);\n</code></pre>"},{"location":"claudie-workflow/claudie-workflow/#flow_5","title":"Flow","text":"<ul> <li>Receives a <code>config</code> object from Builder</li> <li>Generates KubeOne manifest based on the <code>desiredState</code></li> <li>Uses KubeOne to provision a kubernetes cluster</li> <li>Updates the <code>currentState</code> in the <code>config</code></li> </ul>"},{"location":"claudie-workflow/claudie-workflow/#kuber","title":"Kuber","text":"<p>Kuber manipulates the cluster resources using <code>kubectl</code>.</p>"},{"location":"claudie-workflow/claudie-workflow/#api_6","title":"API","text":"<pre><code>  // RemoveLbScrapeConfig removes scrape config for every LB detached from this cluster.\nrpc RemoveLbScrapeConfig(RemoveLbScrapeConfigRequest) returns (RemoveLbScrapeConfigResponse);\n// StoreLbScrapeConfig stores scrape config for every LB attached to this cluster.\nrpc StoreLbScrapeConfig(StoreLbScrapeConfigRequest) returns (StoreLbScrapeConfigResponse);\n// StoreClusterMetadata creates a secret, which holds the private key and a list of public IP addresses of the cluster supplied.\nrpc StoreClusterMetadata(StoreClusterMetadataRequest) returns (StoreClusterMetadataResponse);\n// DeleteClusterMetadata deletes the secret holding the private key and public IP addresses of the cluster supplied.\nrpc DeleteClusterMetadata(DeleteClusterMetadataRequest) returns (DeleteClusterMetadataResponse);\n// SetUpStorage installs Longhorn into the cluster.\nrpc SetUpStorage(SetUpStorageRequest) returns (SetUpStorageResponse); // StoreKubeconfig creates a secret, which holds the kubeconfig of a Claudie-created cluster.\nrpc StoreKubeconfig(StoreKubeconfigRequest) returns (StoreKubeconfigResponse);\n// DeleteKubeconfig removes the secret that holds the kubeconfig of a Claudie-created cluster.\nrpc DeleteKubeconfig(DeleteKubeconfigRequest) returns (DeleteKubeconfigResponse);\n// DeleteNodes deletes the specified nodes from a k8s cluster.\nrpc DeleteNodes(DeleteNodesRequest) returns (DeleteNodesResponse);\n// PatchNodes uses kubectl patch to change the node manifest.\nrpc PatchNodes(PatchNodeTemplateRequest) returns (PatchNodeTemplateResponse);\n// SetUpClusterAutoscaler deploys Cluster Autoscaler and Autoscaler Adapter for every cluster specified.\nrpc SetUpClusterAutoscaler(SetUpClusterAutoscalerRequest) returns (SetUpClusterAutoscalerResponse);\n// DestroyClusterAutoscaler deletes Cluster Autoscaler and Autoscaler Adapter for every cluster specified.\nrpc DestroyClusterAutoscaler(DestroyClusterAutoscalerRequest) returns (DestroyClusterAutoscalerResponse);\n// PatchClusterInfoConfigMap updates the cluster-info config map in the kube-public namespace with the new\n// kubeconfig. This needs to be done after an api endpoint change as the config map in the kube-public namespace\n// is used by kubeadm when joining.\nrpc PatchClusterInfoConfigMap(PatchClusterInfoConfigMapRequest) returns (PatchClusterInfoConfigMapResponse);\n</code></pre>"},{"location":"claudie-workflow/claudie-workflow/#flow_6","title":"Flow","text":"<ul> <li>Recieves a <code>config</code> from Builder for <code>PatchClusterInfoConfigMap</code></li> <li>updatedes kubeconfig to reflect the new changed endpoint.</li> <li>Receives a <code>config</code> from Builder for <code>SetUpStorage()</code></li> <li>Applies the <code>longhorn</code> deployment</li> <li>Receives a <code>config</code> from Builder for <code>StoreKubeconfig()</code></li> <li>Creates a kubernetes secret that holds the kubeconfig of the Claudie-created cluster</li> <li>Receives a <code>config</code> from Builder for <code>StoreMetadata()</code></li> <li>Creates a kubernetes secret that holds the node metadata of the Claudie-created cluster</li> <li>Receives a <code>config</code> from Builder for <code>StoreLbScrapeConfig()</code></li> <li>Stores scrape config for any LB attached to the Claudie-made cluster.</li> <li>Receives a <code>config</code> from Builder for <code>PatchNodes()</code></li> <li>Patches the node manifests of the Claudie-made cluster.</li> <li>Upon infrastructure deletion request, Kuber deletes the kubeconfig secret, metadata secret, scrape configs and autoscaler of the cluster being deleted</li> </ul>"},{"location":"claudie-workflow/claudie-workflow/#frontend","title":"Frontend","text":"<p>Frontend is a layer between the user and Claudie. New manifests are added as secrets into the kubernetes cluster where Frontend pulls them and saves them to Claudie.</p>"},{"location":"claudie-workflow/claudie-workflow/#api_7","title":"API","text":"<p>This service is a gRPC client, thus it does not provide any API</p>"},{"location":"claudie-workflow/claudie-workflow/#flow_7","title":"Flow","text":"<ul> <li>User applies a new secret holding a manifest</li> <li>Frontend detects it and processes the created/modified input manifest</li> <li>Upon deletion of user-created secrets, Frontend initiates a deletion process of the manifest</li> </ul>"},{"location":"contributing/contributing/","title":"Contributing","text":""},{"location":"contributing/contributing/#bug-reports","title":"Bug reports","text":"<p>When you encounter a bug, please create a new issue and use our bug template. Before you submit, please check:</p> <ul> <li>...that the issue you want to open is not a duplicate</li> <li>...that you submitted the logs/screenshots of any errors and a concise way to reproduce the issue</li> <li>...the input manifest you used </li> </ul> <p>be careful not to include your cloud credentials</p>"},{"location":"contributing/release/","title":"How to release a new version of Claudie","text":"<p>The release process of Claudie consists of a few manual steps and a few automated steps.</p>"},{"location":"contributing/release/#manual-steps","title":"Manual steps","text":"<p>Whoever is responsible for creating a new release has to:</p> <ol> <li>Write a new entry to a relevant Changelog document</li> <li>Add release notes to the Releases page</li> <li>Publish a release</li> </ol>"},{"location":"contributing/release/#automated-steps","title":"Automated steps","text":"<p>After a new release is published, a release pipeline runs, which will:</p> <ol> <li>Build new images tagged with the release tag</li> <li>Push them to the container registry where anyone can pull them</li> <li>Add Claudie manifest files to the release assets, with image tags referencing this release</li> </ol>"},{"location":"crud/crud/","title":"CRUD for Claudie","text":"<p>This document describes how the user manages/communicates with Claudie deployed in a Kubernetes cluster.</p> <p>Claudie has a component called Frontend, which functions like an entrypoint to Claudie. Frontend uses k8s Watch API to continuously pull secrets with a label <code>claudie.io/input-manifest</code> and save them to Claudie database.</p>"},{"location":"crud/crud/#create","title":"Create","text":"<p>In order to create (apply) a new input manifest, the user needs to create a new secret in the namespace where Claudie is deployed. This secret needs needs to have:</p> <ul> <li>a label <code>claudie.io/input-manifest</code></li> <li>a unique field name</li> <li>IMPORTANT: We highly recommend to have single input manifest per secret.</li> </ul>"},{"location":"crud/crud/#example","title":"Example","text":"<p>If you define an input manifest called <code>claudie-manifest.yaml</code> (see the example here) and apply it by:</p> <ol> <li> <p>Creating the secret by running</p> <pre><code>kubectl create secret generic input-manifest --from-file=input-manifest.yaml -n claudie\n</code></pre> </li> <li> <p>Labeling the secret with label <code>claudie.io/input-manifest</code> by running</p> <pre><code>kubectl label secret input-manifest claudie.io/input-manifest=my-fancy-manifest -n claudie\n</code></pre> </li> </ol>"},{"location":"crud/crud/#read","title":"Read","text":"<p>The user and Claudie both share a single \"source of truth\" for the input manifests - Kubernetes secrets. Created in the Claudie namespace, they are accessible by both the user and Claudie. This makes users store input manifests in an IaC manner and can easily be configured for GitOps synchronization (i.e. via FluxCD).</p>"},{"location":"crud/crud/#update","title":"Update","text":"<p>When you want to update the input manifest, you can edit/reapply the secret with the updated input manifest inside of it (the secret name and the data field name will stay the same). Frontend notices the change in the secret data and subsequently notifies the Claudie about change made to the input manifest.</p>"},{"location":"crud/crud/#delete","title":"Delete","text":"<p>If you wish to destroy your cluster along with the infrastructure, you can remove the cluster definition block from the input-manifest and update the k8s secret accordingly. If you wish to delete all of the clusters defined in an input-manifest, you simply need to delete the k8s secret containing the manifest. Both events will trigger the deletion process. This process deletes the current infrastructure and it also deletes all data related to the particular input manifest.</p>"},{"location":"crud/crud/#outputs","title":"Outputs","text":"<p>Claudie outputs two secrets in the namespace where it is deployed, after a successful run of the (input) manifest:</p> <ul> <li>kubeconfig,</li> <li>cluster metadata to your clusters.</li> </ul> <p>The names of the secrets are derived as follows: <code>&lt;cluster-name&gt;-&lt;cluster-hash&gt;-{kubeconfig,metadata}</code>. The secrets can be accessed by printing and <code>base64</code>-decoding them.</p> <p>Example of how to decode a kubeconfig from secret:</p> <pre><code>kubectl get secrets -n claudie &lt;cluster-name&gt;-&lt;cluster-hash&gt;-kubeconfig -o jsonpath='{.data.kubeconfig}' | base64 -d &gt; your_kubeconfig.yaml\n</code></pre>"},{"location":"docs-guides/deployment-workflow/","title":"Deployment workflow","text":"<p>Our documentation is hosted on GitHub Pages. Whenever a new push to gh-pages branch happens, it will deploy a new version of the doc. All the commits and pushes to this branch are automated through our release-docs.yml pipeline with the usage of mike tool.</p> <p>That's also the reason, why we do not recommend making any manual changes in gh-pages branch. However, in case you have to, use the commands below.</p> <p>To generate a new version of the docs you can run the command below.</p> <pre><code>mike deploy &lt;version&gt;\n</code></pre> <p>If you would like to deploy that version to our production, you have to run:</p> <pre><code>mike deploy &lt;version&gt; --push\n</code></pre> <p>If you want to make that version the default one, you should run this command:</p> <pre><code>mike set-default &lt;version&gt;\n</code></pre> <p>In case you want to deploy a docs from some older GitHub tags to production, you will have to:</p> <ul> <li><code>git checkout tags/&lt;tag&gt; -b &lt;branch&gt;</code></li> <li><code>create mkdocs.yml</code></li> <li><code>python3 -m venv ./venv</code></li> <li><code>source ./venv/bin/activate</code></li> <li><code>pip install -r requirements.txt</code></li> <li><code>mike deploy &lt;version&gt;</code> --push`</li> </ul> <p>In case the release-docs.yml fails, you can deploy the new version manually by following this steps:</p> <ul> <li><code>git checkout -b &lt;branch&gt;</code></li> <li><code>python3 -m venv ./venv</code></li> <li><code>source ./venv/bin/activate</code></li> <li><code>pip install -r requirements.txt</code></li> <li><code>mike deploy &lt;version&gt; latest --push</code></li> </ul> <p>:warning: Don't forget to use the <code>latest</code> tag in the last command, because otherwise the new version will not be loaded as default one, when visiting docs.claudie.io</p> <p>Find more about how to work with mike.</p>"},{"location":"docs-guides/development/","title":"Development","text":"<p>First of all, it is worth to mention, that we are using MkDocs to generate HTML documents from markdown ones. To make our documentation prettier, we have used Material theme for MkDocs. Regarding the version of our docs we are using mike.</p>"},{"location":"docs-guides/development/#how-to-run","title":"How to run","text":"<p>First of all you have to install the dependencies from requirements.txt in your local machine. However before doing that we recommend creating a virtual enviroment by running the command below.</p> <pre><code>python3 -m venv ./venv\n</code></pre> <p>After that you want to activate that newly create virtual environment by running:</p> <pre><code>source ./venv/bin/activate\n</code></pre> <p>Now, we can install the docs dependencies, which we mentioned before.</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>After successfull instalation, you can run command below, which generates HTML files for the docs and host in on your local server.</p> <pre><code>mkdocs serve\n</code></pre>"},{"location":"docs-guides/development/#how-to-test-changes","title":"How to test changes","text":"<p>Whenever you make some changes in docs folder or in mkdocs.yml file, you can see if the changes were applied as you expected by running the command below, which starts the server with newly generated docs.</p> <pre><code>mkdocs serve\n</code></pre> <p>Using this cmd you will not see the docs versioning, because we are using mike tool for this.</p> <p>In case you want to test the docs versioning, you will have to run:</p> <pre><code>mike serve\n</code></pre> <p>Keep in mind, that mike takes the docs versions from gh-pages branch. That means, you will not be able to see your changes, in case you didn't run the command below before.</p> <pre><code>mike deploy &lt;version&gt;\n</code></pre> <p>Be careful, because this command creates a new version of the docs in your local gh-pages branch.</p>"},{"location":"getting-started/get-started-using-claudie/","title":"Get started using Claudie","text":""},{"location":"getting-started/get-started-using-claudie/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, please make sure you have the following prerequisite installed and set up:</p> <ol> <li>Cert-manager:  Claudie requires cert-manager to be installed in your Kubernetes Manadger cluster, for managing webhooks certificates and ca-injection. To install cert-manager, run the following command:     <pre><code>kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.12.0/cert-manager.yaml\n</code></pre> For more advanced cert-manager installation, please refer to the official documentation.</li> </ol>"},{"location":"getting-started/get-started-using-claudie/#first-steps","title":"First steps","text":"<ol> <li>Download and extract manifests of the lates release from our release page.    <pre><code>wget https://github.com/berops/claudie/releases/latest/download/claudie.zip &amp;&amp; unzip claudie.zip -d claudie\n</code></pre></li> <li>Deploy Claudie Kubernetes manifests/claudie into a Kubernetes cluster:    <pre><code>kubectl apply -k claudie\n</code></pre></li> <li>Provide your own manifest via a Kubernetes Secret, but before that please have a look at our reference example input manifest to explore what's possible.</li> </ol> <p>To see in detail how to correctly apply the manifest into Claudie, please refer to the CRUD document.</p> <p>After the input manifest is successfully applied, the kubeconfig to your newly built clusters is output as a secret in the <code>claudie</code> namespace with a name in the form of <code>&lt;cluster-name&gt;-&lt;cluster-hash&gt;-kubeconfig</code>.</p>"},{"location":"input-manifest/example/","title":"Example yaml file","text":"example.yaml<pre><code>name: ExampleManifest\n\n# Providers field is used for defining the providers.\n# Every supported provider has an example in this input manifest.\n# All of the sensitive credentials are dummy values, similar to the ones user will supply.\nproviders:\n# Hetzner DNS provider.\n# Definition specification:\n# - name:         # Name of this provider instance.\n#   apiToken:     # API token of this provider instance.\n# Example definition:\nhetznerdns:\n- name: hetznerdns-1\napiToken: kslISA878a6etYAfXYcg5iYyrFGNlCxc\n# Cloudflare DNS provider.\n# Definition specification:\n# - name:         # Name of this provider instance.\n#   apiToken:     # API token of this provider instance.\n# Example definition:\ncloudflare:\n- name: cloudflare-1\napiToken: kslISA878a6etYAfXYcg5iYyrFGNlCxc\n# Hetzner Cloud provider.\n# Definition specification:\n# - name:           # Name of this provider instance.\n#   credentials:    # API token of this provider instance.\n# Example definition:\nhetzner:\n- name: hetzner-1\ncredentials: kslISA878a6etYAfXYcg5iYyrFGNlCxcICo060HVEygjFs21nske76ksjKko21lp\n# GCP cloud provider.\n# Definition specification:\n# - name:           # Name of this provider instance.\n#   credentials:    # Service account key in JSON format.\n#   gcpProject:     # GCP project of this service account.\n# Example definition:\ngcp:\n- name: gcp-1\ncredentials: |\n{\n\"type\": \"service_account\",\n\"project_id\": \"project-claudie\",\n\"private_key_id\": \"bskdlo875s9087394763eb84e407903lskdimp439\",\n\"private_key\": \"-----BEGIN PRIVATE KEY-----\\nSKLOosKJUSDANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhki\\n-----END PRIVATE KEY-----\\n\",\n\"client_email\": \"claudie@project-claudie-123456.iam.gserviceaccount.com\",\n\"client_id\": \"109876543211234567890\",\"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n\"token_uri\": \"https://oauth2.googleapis.com/token\",\n\"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n\"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/claudie%40claudie-project-123456.iam.gserviceaccount.com\"\n}\ngcpProject: project-id\n# OCI cloud provider.\n# Definition specification:\n# - name:            # Name of this provider instance.\n#   privateKey:      # Private key of this user account.\n#   keyFingerprint:  # Fingerprint of the key pair.\n#   tenancyOcid:     # OCID of the tenancy.\n#   userOcid:        # OCID of the user.\n#   compartmentOcid: # OCID of the compartment, where resources will be created.\n# Example definition:\noci:\n- name: oci-1\nprivateKey: |\n-----BEGIN RSA PRIVATE KEY-----\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/==\n-----END RSA PRIVATE KEY-----\nkeyFingerprint: ab:cd:3f:34:33:22:32:34:54:54:45:76:76:78:98:aa\ntenancyOcid: ocid2.tenancy.oc2..aaaaaaaayrsfvlvxc34o060kfdygsds21nske76ksjkko21lpsdfsfsgbrtghs\nuserOcid: ocid2.user.oc2..aaaaaaaaaanyrsfvlvxc34o060kfdygsds21nske76ksjkko21lpsdfsf\ncompartmentOcid: ocid2.compartment.oc2..aaaaaaaaa2rsfvlvxc34o060kfdygsds21nske76ksjkko21lpsdfsf\n# AWS cloud provider.\n# Definition specification:\n# - name:            # Name of this provider instance.\n#   accessKey:       # Access key for the service account\n#   secretKey:       # Secret key for the service account.\n# Example definition:\naws:\n- name: aws-1\naccessKey: SLDUTKSHFDMSJKDIALASSD\nsecretKey: iuhbOIJN+oin/olikDSadsnoiSVSDsacoinOUSHD\n# Azure cloud provider.\n# Definition specification:\n# - name:            # Name of this provider instance.\n#   clientSecret:    # Service principal secret.\n#   subscriptionId:  # ID of the subscription.\n#   tenantId:        # ID of the tenancy.\n#   clientId:        # ID of the client.\n# Example definition:\nazure:\n- name: azure-1\nclientSecret: Abcd~EFg~H6Ijkls~ABC15sEFGK54s78X~Olk9\nsubscriptionId: 6a4dfsg7-sd4v-f4ad-dsva-ad4v616fd512\ntenantId: 54cdafa5-sdvs-45ds-546s-df651sfdt614\nclientId: 0255sc23-76we-87g6-964f-abc1def2gh3l\n\n# Nodepools field is used for defining the nodepool specification.\n# You can think of them as a blueprints, not actual nodepools that will be created.\nnodePools:\n# Dynamic nodepools are created by Claudie, in one of the cloud providers specified.\n# Definition specification:\n# dynamic:\n#   - name:             # Name of the nodepool, which is used as a refference to it. Needs to be unique.\n#     providerSpec:     # Provider specification for this nodepool.\n#       name:           # Name of the provider instance, referencing one of the providers define above.\n#       region:         # Region of the nodepool.\n#       zone:           # Zone of the nodepool.\n#     count:            # Static number of nodes in this nodepool.\n#     serverType:       # Machine type of the nodes in this nodepool.\n#     image:            # OS image of the nodes in the nodepool.\n#     storageDiskSize:  # Disk size of the storage disk for compute nodepool.\n#     autoscaler:       # Autoscaler configuration. Mutually exclusive with Count.\n#       min:            # Minimum number of nodes in nodepool.\n#       max:            # Maximum number of nodes in nodepool.\n#\n# Example definitions for each provider\ndynamic:\n- name: control-hetzner\nproviderSpec:\nname: hetzner-1\nregion: hel1\nzone: hel1-dc2\ncount: 3\nserverType: cpx11\nimage: ubuntu-22.04\n\n- name: compute-hetzner\nproviderSpec:\nname: hetzner-1\nregion: hel1\nzone: hel1-dc2\ncount: 2\nserverType: cpx11\nimage: ubuntu-22.04\nstorageDiskSize: 50\n\n- name: compute-hetzner-autoscaled\nproviderSpec:\nname: hetzner-1\nregion: hel1\nzone: hel1-dc2\nserverType: cpx11\nimage: ubuntu-22.04\nstorageDiskSize: 50\nautoscaler:\nmin: 1\nmax: 5\n\n- name: control-gcp\nproviderSpec:\nname: gcp-1\nregion: europe-west1\nzone: europe-west1-c\ncount: 3\nserverType: e2-medium\nimage: ubuntu-os-cloud/ubuntu-2204-jammy-v20221206\n\n- name: compute-gcp\nproviderSpec:\nname: gcp-1\nregion: europe-west1\nzone: europe-west1-c\ncount: 2\nserverType: e2-small\nimage: ubuntu-os-cloud/ubuntu-2204-jammy-v20221206\nstorageDiskSize: 50\n\n- name: control-oci\nproviderSpec:\nname: oci-1\nregion: eu-milan-1\nzone: hsVQ:EU-MILAN-1-AD-1\ncount: 3\nserverType: VM.Standard2.1\nimage: ocid1.image.oc1.eu-frankfurt-1.aaaaaaaavvsjwcjstxt4sb25na65yx6i34bzdy5oess3pkgwyfa4hxmzpqeq\n\n- name: compute-oci\nproviderSpec:\nname: oci-1\nregion: eu-milan-1\nzone: hsVQ:EU-MILAN-1-AD-1\ncount: 2\nserverType: VM.Standard2.1\nimage: ocid1.image.oc1.eu-frankfurt-1.aaaaaaaavvsjwcjstxt4sb25na65yx6i34bzdy5oess3pkgwyfa4hxmzpqeq\nstorageDiskSize: 50\n\n- name: control-aws\nproviderSpec:\nname: aws-1\nregion: eu-central-1\nzone: eu-central-1c\ncount: 2\nserverType: t3.medium\nimage: ami-0965bd5ba4d59211c\n\n- name: compute-aws\nproviderSpec:\nname: aws-1\nregion: eu-central-1\nzone: eu-central-1c\ncount: 2\nserverType: t3.medium\nimage: ami-0965bd5ba4d59211c\nstorageDiskSize: 50\n\n- name: control-azure\nproviderSpec:\nname: azure-1\nregion: West Europe\nzone: 1\ncount: 2\nserverType: Standard_B2s\nimage: Canonical:0001-com-ubuntu-minimal-jammy:minimal-22_04-lts:22.04.202212120\n\n- name: compute-azure\nproviderSpec:\nname: azure-1\nregion: West Europe\nzone: 1\ncount: 2\nserverType: Standard_B2s\nimage: Canonical:0001-com-ubuntu-minimal-jammy:minimal-22_04-lts:22.04.202212120\nstorageDiskSize: 50\n\n- name: loadbalancer-1\nprovider:\nproviderSpec:\nname: gcp-1\nregion: europe-west1\nzone: europe-west1-c\ncount: 2\nserverType: e2-small\nimage: ubuntu-os-cloud/ubuntu-2004-focal-v20220610\n\n- name: loadbalancer-2\nproviderSpec:\nname: hetzner-1\nregion: hel1\nzone: hel1-dc2\ncount: 2\nserverType: cpx11\nimage: ubuntu-20.04\n\n# Kubernetes field is used to define the kubernetes clusters.\n# Definition specification:\n#\n# clusters:\n#   - name:           # Name of the cluster. The name will be appended to the created node name.\n#     version:        # Kubernetes version in semver scheme, must be supported by KubeOne.\n#     network:        # Private network IP range.\n#     pools:          # Nodepool names which cluster will be composed of. User can reuse same nodepool specification on multiple clusters.\n#       control:      # List of nodepool names, which will be used as control nodes.\n#       compute:      # List of nodepool names, which will be used as compute nodes.\n#\n# Example definitions:\nkubernetes:\nclusters:\n- name: dev-cluster\nversion: v1.24.0\nnetwork: 192.168.2.0/24\npools:\ncontrol:\n- control-hetzner\n- control-gcp\ncompute:\n- compute-hetzner\n- compute-gcp\n- compute-azure\n\n- name: prod-cluster\nversion: v1.24.0\nnetwork: 192.168.2.0/24\npools:\ncontrol:\n- control-hetzner\n- control-gcp\n- control-oci\n- control-aws\n- control-azure\ncompute:\n- compute-hetzner\n- compute-gcp\n- compute-oci\n- compute-aws\n- compute-azure\n\n# Loadbalancers field defines loadbalancers used for the kubernetes clusters and roles for the loadbalancers.\n# Definition specification for role:\n#\n# roles:\n#   - name:         # Name of the role, used as a reference later. Must be unique.\n#     protocol:     # Protocol, this role will use.\n#     port:         # Port, where trafic will be coming.\n#     targetPort:   # Port, where loadbalancer will forward traffic to.\n#     target:       # Targeted nodes on kubernetes cluster. Can be \"k8sControlPlane\", \"k8sComputePlane\" or \"k8sAllNodes\".\n#\n# Definition specification for loadbalancer:\n#\n# clusters:\n#   - name:         # Loadbalancer cluster name\n#     roles:        # List of role names this loadbalancer will fullfil.\n#     dns:          # DNS specification, where DNS records will be created.\n#       dnsZone:    # DNS zone name in your provider.\n#       provider:   # Provider name for the DNS.\n#       hostname:   # Hostname for the DNS record. Keep in mind the zone will be included automaticaly. If left empty the Claudie will create random hash as a hostname.\n#     targetedK8s:  # Name of the targeted kubernetes cluster\n#     pools:        # List of nodepool names used for loadbalancer\n# Example definitons:\nloadBalancers:\nroles:\n- name: apiserver\nprotocol: tcp\nport: 6443\ntargetPort: 6443\ntarget: k8sControlPlane\n\nclusters:\n- name: apiserver-lb-dev\nroles:\n- apiserver\ndns:\ndnsZone: dns-zone\nprovider: hetznerdns-1\ntargetedK8s: dev-cluster\npools:\n- loadbalancer-1\n- name: apiserver-lb-prod\nroles:\n- apiserver\ndns:\ndnsZone: dns-zone\nprovider: cloudflare-1\nhostname: my.fancy.url\ntargetedK8s: prod-cluster\npools:\n- loadbalancer-2\n</code></pre>"},{"location":"input-manifest/input-manifest/","title":"Input manifest","text":""},{"location":"input-manifest/input-manifest/#manifest","title":"Manifest","text":"<p>Manifest is a definition of the user's infrastructure. It contains cloud provider specification, nodepool specification, Kubernetes and loadbalancer clusters.</p> <ul> <li><code>name</code></li> </ul> <p>Name of the manifest. Must be unique across all manifests of the Claudie instance.</p> <ul> <li><code>providers</code> Providers</li> </ul> <p>Defines all your cloud provider configuration that will be used while infrastructure provisioning.</p> <ul> <li><code>nodepools</code> Nodepools</li> </ul> <p>Describes nodepools used for either kubernetes clusters or loadbalancer cluster defined in this manifest.</p> <ul> <li><code>kubernetes</code> Kubernetes</li> </ul> <p>List of Kubernetes cluster this manifest will manage.</p> <ul> <li><code>loadBalancers</code> Loadbalancer</li> </ul> <p>List of loadbalancer clusters the Kubernetes clusters may use.</p>"},{"location":"input-manifest/input-manifest/#providers","title":"Providers","text":"<p>Contains configurations for supported cloud providers. At least one provider needs to be defined.</p> <ul> <li><code>gcp</code> GCP</li> </ul> <p>List of configuration options for Google Cloud. This field is optional.</p> <ul> <li><code>hetzner</code> Hetzner</li> </ul> <p>List of configuration options for Hetzner Cloud . This field is optional.</p> <ul> <li><code>oci</code> OCI</li> </ul> <p>List of configuration options for Oracle Cloud Infrastructure. This field is optional.</p> <ul> <li><code>aws</code> AWS</li> </ul> <p>List of configuration options for Amazon Web Services. This field is optional.</p> <ul> <li><code>azure</code> Azure</li> </ul> <p>List of configuration options for Azure. This field is optional.</p> <ul> <li><code>cloudflare</code> Cloudflare</li> </ul> <p>List of Cloudflare configuration for Cloudflare. This field is optional.</p> <ul> <li><code>hetznerdns</code> HetznerDNS</li> </ul> <p>List of HetznerDNS configuration for HetznerDNS. This field is optional.</p> <p>Support for more cloud providers is in the roadmap.</p>"},{"location":"input-manifest/input-manifest/#cloudflare","title":"Cloudflare","text":"<p>Collection of data defining Cloudflare provider configuration.</p> <p>To find out how to configure Cloudflare follow the instructions here</p> <ul> <li><code>name</code></li> </ul> <p>Name of the provider. Used as a reference further in the input manifest. Should be unique for each provider spec across all the providers.</p> <ul> <li><code>apiToken</code></li> </ul> <p>Credentials for the provider (API token).</p>"},{"location":"input-manifest/input-manifest/#hetznerdns","title":"HetznerDNS","text":"<p>Collection of data defining HetznerDNS provider configuration.</p> <p>To find out how to configure HetznerDNS follow the instructions here</p> <ul> <li><code>name</code></li> </ul> <p>Name of the provider. Used as a reference further in the input manifest. Should be unique for each provider spec across all the providers.</p> <ul> <li><code>apiToken</code></li> </ul> <p>Credentials for the provider (API token).</p>"},{"location":"input-manifest/input-manifest/#gcp","title":"GCP","text":"<p>Collection of data defining GCP cloud provider configuration.</p> <p>To find out how to configure GCP provider and service account, follow the instructions here.</p> <ul> <li><code>name</code></li> </ul> <p>Name of the provider. Used as a reference further in the input manifest. Should be unique for each provider spec across all the cloud providers.</p> <ul> <li><code>credentials</code></li> </ul> <p>Credentials for the provider. Stringified JSON service account key.</p> <ul> <li><code>gcpProject</code></li> </ul> <p>project id of an already existing GCP project.</p>"},{"location":"input-manifest/input-manifest/#hetzner","title":"Hetzner","text":"<p>Collection of data defining Hetzner cloud provider configuration.</p> <p>To find out how to configure Hetzner provider and service account, follow the instructions here.</p> <ul> <li><code>name</code></li> </ul> <p>Name of the provider spec. Used as a reference further in the input manifest. Should be unique for each provider spec across all the cloud providers.</p> <ul> <li><code>credentials</code></li> </ul> <p>Credentials for the provider (API token).</p>"},{"location":"input-manifest/input-manifest/#oci","title":"OCI","text":"<p>Collection of data defining OCI cloud provider configuration.</p> <p>To find out how to configure OCI provider and service account, follow the instructions here.</p> <ul> <li><code>name</code></li> </ul> <p>Name of the provider spec. Used as a reference further in the input manifest. Should be unique for each provider spec across all the cloud providers.</p> <ul> <li><code>privateKey</code></li> </ul> <p>Private key used to authenticate to the OCI.</p> <ul> <li><code>keyFingerprint</code></li> </ul> <p>Fingerprint of the user-supplied private key.</p> <ul> <li><code>tenancyOcid</code></li> </ul> <p>OCID of the tenancy where <code>privateKey</code> is added as an API key</p> <ul> <li><code>userOcid</code></li> </ul> <p>OCID of the user in the supplied tenancy</p> <ul> <li><code>compartmentOcid</code></li> </ul> <p>OCID of the compartment where VMs/VCNs/... will be created</p>"},{"location":"input-manifest/input-manifest/#aws","title":"AWS","text":"<p>Collection of data defining AWS cloud provider configuration.</p> <p>To find out how to configure AWS provider and service account, follow the instructions here.</p> <ul> <li><code>name</code></li> </ul> <p>Name of the provider spec. Used as a reference further in the input manifest. Should be unique for each provider spec across all the cloud providers.</p> <ul> <li><code>accessKey</code></li> </ul> <p>Access key ID for your AWS account.</p> <ul> <li><code>secretKey</code></li> </ul> <p>Secret key for the Access key specified above.</p>"},{"location":"input-manifest/input-manifest/#azure","title":"Azure","text":"<p>Collection of data defining Azure cloud provider configuration.</p> <p>To find out how to configure Azure provider and service account, follow the instructions here.</p> <ul> <li><code>name</code></li> </ul> <p>Name of the provider spec. Used as a reference further in the input manifest. Should be unique for each provider spec across all the cloud providers.</p> <ul> <li><code>subscriptionId</code></li> </ul> <p>Subscription ID of your subscription in Azure.</p> <ul> <li><code>tenantId</code></li> </ul> <p>Tenant ID of your tenancy in Azure.</p> <ul> <li><code>clientId</code></li> </ul> <p>Client ID of your client. The Claudie is design to use a service principal with appropriate permissions.</p> <ul> <li><code>clientSecret</code></li> </ul> <p>Client secret generated for your client.</p>"},{"location":"input-manifest/input-manifest/#nodepools","title":"Nodepools","text":"<p>Collection of static and dynamic nodepool specification, to be referenced in the <code>kubernetes</code> or <code>loadBalancer</code> clusters.</p> <ul> <li><code>dynamic</code> Dynamic</li> </ul> <p>List of dynamically to-be-created nodepools of not yet existing machines, used for Kubernetes or loadbalancer clusters.</p> <p>These are only blueprints, and will only be created per reference in <code>kubernetes</code> or <code>loadBalancer</code> clusters. E.g. if the nodepool isn't used, it won't even be created. Or if the same nodepool is used in two different clusters, it will be created twice. In OOP analogy, a dynamic nodepool would be a class that would get instantiated <code>N &gt;= 0</code> times depending on which clusters reference it.</p> <ul> <li><code>static</code> [WORK IN PROGRESS]</li> </ul> <p>List of static nodepools of already existing machines, not created by of Claudie, used for Kubernetes or loadbalancer clusters. Typically, these would be on-premises machines.</p>"},{"location":"input-manifest/input-manifest/#dynamic","title":"Dynamic","text":"<p>Dynamic nodepools are defined for cloud provider machines that Claudie is expected to create.</p> <ul> <li><code>name</code></li> </ul> <p>Name of the nodepool. Each nodepool will have a random hash appended to the name, so the whole name will be of format <code>&lt;name&gt;-&lt;hash&gt;</code>.</p> <ul> <li><code>provideSpec</code> Provider spec</li> </ul> <p>Collection of provider data to be used while creating the nodepool.  </p> <ul> <li><code>count</code></li> </ul> <p>Number of the nodes in the nodepool. Mutually exclusive with <code>autoscaler</code>.</p> <ul> <li><code>serverType</code></li> </ul> <p>Type of the machines in the nodepool.</p> <p>Currently, only AMD64 machines are supported.</p> <ul> <li><code>image</code></li> </ul> <p>OS image of the machine.</p> <p>Currently, only Ubuntu 22.04 AMD64 images are supported.</p> <ul> <li><code>storageDiskSize</code></li> </ul> <p>Size of the storage disk on the nodes in the nodepool in <code>GB</code>. The OS disk is created automatically with predefined size of <code>100GB</code> for kubernetes nodes and <code>50GB</code> for Loadbalancer nodes.</p> <p>Default value is <code>50</code>, minimum value is <code>50</code>. Value is used only for compute nodes.</p> <p>This field is optional, however, if compute nodepool does not define it, default value is used for creation of storage disk. Control nodepools and Loadbalancer nodepools ignore this field.</p> <ul> <li><code>autoscaler</code> Autoscaler Configuration</li> </ul> <p>Autoscaler configuration for this nodepool. Mutually exclusive with <code>count</code>.</p>"},{"location":"input-manifest/input-manifest/#provider-spec","title":"Provider Spec","text":"<p>Provider spec is further specification build on top of the data from any of the provider instance. To see simple examples of Provider spec, see the provider examples.</p> <ul> <li><code>name</code></li> </ul> <p>Name of the provider instance specified in providers</p> <ul> <li><code>region</code></li> </ul> <p>Region of the nodepool.</p> <ul> <li><code>zone</code></li> </ul> <p>Zone of the nodepool.</p>"},{"location":"input-manifest/input-manifest/#autoscaler-configuration","title":"Autoscaler Configuration","text":"<p>Autoscaler configuration on per nodepool basis. Defines the number of nodes, autoscaler will scale up or down specific nodepool.</p> <ul> <li><code>min</code></li> </ul> <p>Minimum number of nodes in nodepool.</p> <ul> <li><code>max</code></li> </ul> <p>Maximum number of nodes in nodepool.</p>"},{"location":"input-manifest/input-manifest/#kubernetes","title":"Kubernetes","text":"<p>Defines Kubernetes clusters.</p> <ul> <li><code>clusters</code> Cluster-k8s</li> </ul> <p>List of Kubernetes clusters Claudie will create.</p>"},{"location":"input-manifest/input-manifest/#cluster-k8s","title":"Cluster-k8s","text":"<p>Collection of data used to define a Kubernetes cluster.</p> <ul> <li><code>name</code></li> </ul> <p>Name of the Kubernetes cluster. Each cluster will have a random hash appended to the name, so the whole name will be of format <code>&lt;name&gt;-&lt;hash&gt;</code>.</p> <ul> <li><code>version</code></li> </ul> <p>Kubernetes version of the cluster.</p> <p>Version should be defined in format <code>vX.Y</code>. In terms of supported versions of Kubernetes, Claudie follows <code>kubeone</code> releases and their supported versions. The current <code>kubeone</code> version used in Claudie is <code>1.5</code>. To see the list of supported versions, please refer to <code>kubeone</code> documentation.</p> <ul> <li><code>network</code></li> </ul> <p>Network range for the VPN of the cluster. The value should be defined in format <code>A.B.C.D/mask</code>.</p> <ul> <li><code>pools</code></li> </ul> <p>List of nodepool names this cluster will use. Remember that nodepools defined in nodepools are only \"blueprints\". The actual nodepool will be created once referenced here.</p>"},{"location":"input-manifest/input-manifest/#loadbalancer","title":"LoadBalancer","text":"<p>Defines loadbalancer clusters.</p> <ul> <li><code>roles</code> Role</li> </ul> <p>List of roles loadbalancers use to forward the traffic. Single role can be used in multiple loadbalancer clusters.</p> <ul> <li><code>clusters</code> Cluster-lb</li> </ul> <p>List of loadbalancer clusters used in the Kubernetes clusters defined under clusters.</p>"},{"location":"input-manifest/input-manifest/#role","title":"Role","text":"<p>Role defines a concrete loadbalancer configuration. Single loadbalancer can have multiple roles.</p> <ul> <li><code>name</code></li> </ul> <p>Name of the role. Used as a reference in clusters.</p> <ul> <li><code>protocol</code></li> </ul> <p>Protocol of the rule. Allowed values are:</p> Value Description <code>tcp</code> Role will use TCP protocol <code>udp</code> Role will use UDP protocol <ul> <li><code>port</code></li> </ul> <p>Port of the incoming traffic on the loadbalancer.</p> <ul> <li><code>targetPort</code></li> </ul> <p>Port where loadbalancer forwards the traffic.</p> <ul> <li><code>target</code></li> </ul> <p>Defines a target group of nodes. Allowed values are:</p> Value Description <code>k8sAllNodes</code> All nodes in the cluster <code>k8sControlNodes</code> Only control/master nodes in cluster <code>k8sComputeNodes</code> Only compute/worker nodes in cluster"},{"location":"input-manifest/input-manifest/#cluster-lb","title":"Cluster-lb","text":"<p>Collection of data used to define a loadbalancer cluster.</p> <ul> <li><code>name</code></li> </ul> <p>Name of the loadbalancer.</p> <ul> <li><code>roles</code></li> </ul> <p>List of roles the loadbalancer uses.</p> <ul> <li><code>dns</code> DNS</li> </ul> <p>Specification of the loadbalancer's DNS record.</p> <ul> <li><code>targetedK8s</code></li> </ul> <p>Name of the Kubernetes cluster targetted by this loadbalancer.</p> <ul> <li><code>pools</code></li> </ul> <p>List of nodepool names this loadbalancer will use. Remember, that nodepools defined in nodepools are only \"blueprints\". The actual nodepool will be created once referenced here.</p>"},{"location":"input-manifest/input-manifest/#dns","title":"DNS","text":"<p>Collection of data Claudie uses to create a DNS record for the loadbalancer.</p> <ul> <li><code>dnsZone</code></li> </ul> <p>DNS zone inside of which the records will be created. GCP/AWS/OCI/Azure/Cloudflare/Hetzner DNS zone is accepted</p> <ul> <li><code>provider</code></li> </ul> <p>Name of provider to be used for creating an A record entry in defined DNS zone.</p> <ul> <li><code>hostname</code></li> </ul> <p>Custom hostname for your A record. If left empty, the hostname will be a random hash.</p>"},{"location":"input-manifest/providers/aws/","title":"AWS","text":"<p>In Claudie, the AWS cloud provider requires you to input the credentials as an <code>accessKey</code> and a <code>secretKey</code> which will be linked to the IAM user in your account. It is important, that said IAM user will have sufficient policies attached. This will assure that Claudie will be able to create all resources for your infrastructure.</p>"},{"location":"input-manifest/providers/aws/#dns-requirements","title":"DNS requirements","text":"<p>If your AWS provider will be used for DNS, you need to manually</p> <ul> <li>set up dns zone</li> <li>update domain name server</li> </ul> <p>since Claudie does not support their dynamic creation.</p>"},{"location":"input-manifest/providers/aws/#iam-policies-required-by-claudie","title":"IAM policies required by Claudie","text":"<pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"ec2:*\"\n],\n\"Resource\": \"*\"\n},\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"route53:*\"\n],\n\"Resource\": \"*\"\n}\n]\n}\n</code></pre>"},{"location":"input-manifest/providers/azure/","title":"Azure","text":"<p>In Claudie, Azure cloud provider requires you to input a few variables in order to function properly. These variables are</p> <ul> <li> <p><code>subscriptionId</code> which is the ID to your subscription. Bear in mind that all resources you define needs to be supported by that subscription and will be charged there.</p> </li> <li> <p><code>tenantId</code> which is the ID of your tenant in the active directory.</p> </li> <li> <p><code>clientId</code> which is the ID for your service principal, under the tenancy you specified.</p> </li> <li> <p><code>clientSecret</code> which is the secret, for specified service principal.</p> </li> </ul> <p>Furthermore, service principal has to have a certain role assigned to it. For VM and VPC management it is <code>Virtual Machine Contributor</code> and <code>Network Contributor</code> respectively; and for resource group creation and deletion,the permission are</p> <pre><code>permissions {\nactions = [ \"Microsoft.Resources/subscriptions/resourceGroups/write\",\n\"Microsoft.Resources/subscriptions/resourceGroups/delete\",\n]\n}\n</code></pre>"},{"location":"input-manifest/providers/azure/#dns-requirements","title":"DNS requirements","text":"<p>If your Azure provider will be used for DNS, you need to manually</p> <ul> <li>set up dns zone</li> <li>update domain name server</li> </ul> <p>since Claudie does not support their dynamic creation.</p>"},{"location":"input-manifest/providers/cloudflare/","title":"Cloudflare","text":"<p>For using your Cloudflare provider for DNS, you need to manually</p> <ul> <li>set up zone</li> <li>Update your name servers for you domain with the NS records from the created DNS zone.</li> </ul> <p>since Claudie does not support their dynamic creation.</p>"},{"location":"input-manifest/providers/cloudflare/#iam-policies-required-by-claudie","title":"IAM policies required by Claudie","text":"<p>The token generated by Cloudflare for you zone has to have this policies</p> <pre><code>Zone:Read, DNS:Read, DNS:Edit\n</code></pre>"},{"location":"input-manifest/providers/gcp/","title":"GCP","text":"<p>In Claudie, GCP cloud provider requires you to input <code>credentials</code> as well as specific project where the account resides. The credentials are in form of an account key in JSON. It is important, that account has sufficient IAM roles attached to it, so Claudie can create all resources for your infrastructure.</p> <p>Furthermore, your project should enable a few API, namely</p> <ul> <li><code>Compute Engine API</code></li> <li><code>Cloud DNS API</code></li> </ul> <p>when project will be used for Loadbalancers DNS</p>"},{"location":"input-manifest/providers/gcp/#dns-requirements","title":"DNS requirements","text":"<p>If your GCP provider will be used for DNS, you need to manually</p> <ul> <li>set up dns zone</li> <li>update domain name server</li> </ul> <p>since Claudie does not support their dynamic creation.</p>"},{"location":"input-manifest/providers/gcp/#iam-policies-required-by-claudie","title":"IAM policies required by Claudie","text":"<ul> <li>for infrastructure creation</li> <li> <p><code>roles/roles/compute.admin</code></p> </li> <li> <p>for DNS</p> </li> <li><code>roles/dns.admin</code></li> </ul>"},{"location":"input-manifest/providers/hetzner/","title":"Hetzner","text":"<p>In Claudie, the Hetzner cloud provider requires you to input the credentials which  contain the API token for your Hetzner project. The API key should have <code>Read &amp; Write</code> permissions.</p> <p>To find out how you can create an API token please follow instructions here.</p> <p>For DNS the instructions for the token can be found here</p>"},{"location":"input-manifest/providers/hetzner/#dns-requirements","title":"DNS requirements","text":"<p>If your Hetzner provider will be used for DNS, you need to manually</p> <ul> <li>set up dns zone</li> <li>update domain name server</li> </ul> <p>since Claudie does not support their dynamic creation.</p> <p>Note the provider for DNS is different from that for the Cloud.</p>"},{"location":"input-manifest/providers/oci/","title":"OCI","text":"<p>In Claudie, OCI cloud provider requires you to input few variables which points to an account in your tenancy. To access the account, you need to provider tenancy OCID, user OCID, with user's API private key and fingerprint. Finally, you need to provide compartment OCID, which points to the compartment where resources for your infrastructure will be created.</p> <p>However, in order to create those resources, user needs to be in a group with the sufficient IAM policies.</p>"},{"location":"input-manifest/providers/oci/#dns-requirements","title":"DNS requirements","text":"<p>If your OCI provider will be used for DNS, you need to manually</p> <ul> <li>set up dns zone</li> <li>Update your name servers for you domain with the NS records from the created DNS zone.</li> </ul> <p>since Claudie does not support their dynamic creation.</p>"},{"location":"input-manifest/providers/oci/#iam-policies-required-by-claudie","title":"IAM policies required by Claudie","text":"<pre><code>\"Allow group &lt;GROUP_NAME&gt; to manage instance-family in compartment &lt;COMPARTMENT_NAME&gt;\"\n\"Allow group &lt;GROUP_NAME&gt; to manage volume-family in compartment &lt;COMPARTMENT_NAME&gt;\"\n\"Allow group &lt;GROUP_NAME&gt; to manage virtual-network-family in tenancy\"\n\"Allow group &lt;GROUP_NAME&gt; to manage dns-zones in compartment &lt;COMPARTMENT_NAME&gt;\",\n\"Allow group &lt;GROUP_NAME&gt; to manage dns-records in compartment &lt;COMPARTMENT_NAME&gt;\",\n</code></pre>"},{"location":"input-manifest/providers/examples/aws-input-manifest/","title":"AWS input manifest example","text":""},{"location":"input-manifest/providers/examples/aws-input-manifest/#single-provider-multi-region-cluster","title":"Single provider, multi region cluster","text":"<pre><code>name: AWSExampleManifest\n\nproviders:\naws:\n- name: aws-1\n# Access key to your AWS account.\naccessKey: SLDUTKSHFDMSJKDIALASSD\n# Secret key to your AWS account.\nsecretKey: iuhbOIJN+oin/olikDSadsnoiSVSDsacoinOUSHD\n\nnodePools:\ndynamic:\n- name: control-aws\nproviderSpec:\n# Name of the provider instance.\nname: aws-1\n# Region of the nodepool.\nregion: eu-central-1\n# Availability zone of the nodepool.\nzone: eu-central-1a\ncount: 1\n# Instance type name.\nserverType: t3.medium\n# AMI ID of the image.\n# Make sure to update it according to the region. \nimage: ami-0965bd5ba4d59211c\n\n- name: compute-1-aws\nproviderSpec:\n# Name of the provider instance.\nname: aws-1\n# Region of the nodepool.\nregion: eu-central-2\n# Availability zone of the nodepool.\nzone: eu-central-2a\ncount: 2\n# Instance type name.\nserverType: t3.medium\n# AMI ID of the image.\n# Make sure to update it according to the region. \nimage: ami-0965bd5ba4d59211c\nstorageDiskSize: 50\n\n- name: compute-2-aws\nproviderSpec:\n# Name of the provider instance.\nname: aws-1\n# Region of the nodepool.\nregion: eu-central-3\n# Availability zone of the nodepool.\nzone: eu-central-3a\ncount: 2\n# Instance type name.\nserverType: t3.medium\n# AMI ID of the image.\n# Make sure to update it according to the region. \nimage: ami-0965bd5ba4d59211c\nstorageDiskSize: 50\n\nkubernetes:\nclusters:\n- name: aws-cluster\nversion: v1.23.0\nnetwork: 192.168.2.0/24\npools:\ncontrol:\n- control-aws\ncompute:\n- compute-1-aws\n- compute-2-aws\n</code></pre>"},{"location":"input-manifest/providers/examples/aws-input-manifest/#multi-provider-multi-region-clusters","title":"Multi provider, multi region clusters","text":"<pre><code>name: AWSExampleManifest\n\nproviders:\naws:\n- name: aws-1\n# Access key to your AWS account.\naccessKey: SLDUTKSHFDMSJKDIALASSD\n# Secret key to your AWS account.\nsecretKey: iuhbOIJN+oin/olikDSadsnoiSVSDsacoinOUSHD\n\n- name: aws-2\n# Access key to your AWS account.\naccessKey: ODURNGUISNFAIPUNUGFINB\n# Secret key to your AWS account.\nsecretKey: asduvnva+skd/ounUIBPIUjnpiuBNuNipubnPuip\n\nnodePools:\ndynamic:\n- name: control-aws-1\nproviderSpec:\n# Name of the provider instance.\nname: aws-1\nregion: eu-central-1\n# Availability zone of the nodepool.\nzone: eu-central-1a\ncount: 1\n# Instance type name.\nserverType: t3.medium\n# AMI ID of the image.\n# Make sure to update it according to the region. \nimage: ami-0965bd5ba4d59211c\n\n- name: control-aws-2\nproviderSpec:\n# Name of the provider instance.\nname: aws-2\n# Region of the nodepool.\nregion: eu-north-1\n# Availability zone of the nodepool.\nzone: eu-north-1a\ncount: 2\n# Instance type name.\nserverType: t3.medium\n# AMI ID of the image.\n# Make sure to update it according to the region. \nimage: ami-03df6dea56f8aa618\n\n- name: compute-aws-1\nproviderSpec:\n# Name of the provider instance.\nname: aws-1\n# Region of the nodepool.\nregion: eu-central-2\n# Availability zone of the nodepool.\nzone: eu-central-2a\ncount: 2\n# Instance type name.\nserverType: t3.medium\n# AMI ID of the image.\n# Make sure to update it according to the region. \nimage: ami-0965bd5ba4d59211c\nstorageDiskSize: 50\n\n- name: compute-aws-2\nproviderSpec:\n# Name of the provider instance.\nname: aws-2\n# Region of the nodepool.\nregion: eu-north-3\n# Availability zone of the nodepool.\nzone: eu-north-3a\ncount: 2\n# Instance type name.\nserverType: t3.medium\n# AMI ID of the image.\n# Make sure to update it according to the region. \nimage: ami-03df6dea56f8aa618\nstorageDiskSize: 50\n\nkubernetes:\nclusters:\n- name: aws-cluster\nversion: v1.23.0\nnetwork: 192.168.2.0/24\npools:\ncontrol:\n- control-aws-1\n- control-aws-2\ncompute:\n- compute-aws-1\n- compute-aws-2\n</code></pre>"},{"location":"input-manifest/providers/examples/azure-input-manifest/","title":"Azure input manifest example","text":""},{"location":"input-manifest/providers/examples/azure-input-manifest/#single-provider-multi-region-cluster","title":"Single provider, multi region cluster","text":"<pre><code>name: AzureExampleManifest\n\nproviders:\nazure:\n- name: azure-1\n# Service principal secret.\nclientSecret: Abcd~EFg~H6Ijkls~ABC15sEFGK54s78X~Olk9\n# ID of your subscription.\nsubscriptionId: 6a4dfsg7-sd4v-f4ad-dsva-ad4v616fd512\n# ID of your tenancy.\ntenantId: 54cdafa5-sdvs-45ds-546s-df651sfdt614\n# ID of your service principal.\nclientId: 0255sc23-76we-87g6-964f-abc1def2gh3l\n\nnodePools:\ndynamic:\n- name: control-azure\nproviderSpec:\n# Name of the provider instance.\nname: azure-1\n# Location of the nodepool.\nregion: West Europe\n# Zone of the nodepool.\nzone: 1\ncount: 2\n# VM size name.\nserverType: Standard_B2s\n# URN of the image.\nimage: Canonical:0001-com-ubuntu-minimal-jammy:minimal-22_04-lts:22.04.202212120\n\n- name: compute-1-azure\nproviderSpec:\n# Name of the provider instance.\nname: azure-1\n# Location of the nodepool.\nregion: Germany West Central\n# Zone of the nodepool.\nzone: 1\ncount: 2\n# VM size name.\nserverType: Standard_B2s\n# URN of the image.\nimage: Canonical:0001-com-ubuntu-minimal-jammy:minimal-22_04-lts:22.04.202212120\nstorageDiskSize: 50\n\n- name: compute-2-azure\nproviderSpec:\n# Name of the provider instance.\nname: azure-1\n# Location of the nodepool.\nregion: West Europe\n# Zone of the nodepool.\nzone: 1\ncount: 2\n# VM size name.\nserverType: Standard_B2s\n# URN of the image.\nimage: Canonical:0001-com-ubuntu-minimal-jammy:minimal-22_04-lts:22.04.202212120\nstorageDiskSize: 50\n\nkubernetes:\nclusters:\n- name: azure-cluster\nversion: v1.23.0\nnetwork: 192.168.2.0/24\npools:\ncontrol:\n- control-azure\ncompute:\n- compute-2-azure\n- compute-1-azure\n</code></pre>"},{"location":"input-manifest/providers/examples/azure-input-manifest/#multi-provider-multi-region-clusters","title":"Multi provider, multi region clusters","text":"<pre><code>name: AzureExampleManifest\n\nproviders:\nazure:\n- name: azure-1\n# Service principal secret.\nclientSecret: Abcd~EFg~H6Ijkls~ABC15sEFGK54s78X~Olk9\n# ID of your subscription.\nsubscriptionId: 6a4dfsg7-sd4v-f4ad-dsva-ad4v616fd512\n# ID of your tenancy.\ntenantId: 54cdafa5-sdvs-45ds-546s-df651sfdt614\n# ID of your service principal.\nclientId: 0255sc23-76we-87g6-964f-abc1def2gh3l\n\n- name: azure-2\n# Service principal secret.\nclientSecret: Efgh~ijkL~on43noi~NiuscviBUIds78X~UkL7\n# ID of your subscription.\nsubscriptionId: 0965bd5b-usa3-as3c-ads1-csdaba6fd512\n# ID of your tenancy.\ntenantId: 55safa5d-dsfg-546s-45ds-d51251sfdaba\n# ID of your service principal.\nclientId: 076wsc23-sdv2-09cA-8sd9-oigv23npn1p2\n\nnodePools:\ndynamic:\n- name: control-azure-1\nproviderSpec:\n# Name of the provider instance.\nname: azure-1\n# Location of the nodepool.\nregion: West Europe\n# Zone of the nodepool.\nzone: 1\ncount: 1\n# VM size name.\nserverType: Standard_B2s\n# URN of the image.\nimage: Canonical:0001-com-ubuntu-minimal-jammy:minimal-22_04-lts:22.04.202212120\n\n- name: control-azure-2\nproviderSpec:\n# Name of the provider instance.\nname: azure-2\n# Location of the nodepool.\nregion: Germany West Central\n# Zone of the nodepool.\nzone: 2\ncount: 2\n# VM size name.\nserverType: Standard_B2s\n# URN of the image.\nimage: Canonical:0001-com-ubuntu-minimal-jammy:minimal-22_04-lts:22.04.202212120\n\n- name: compute-azure-1\nproviderSpec:\n# Name of the provider instance.\nname: azure-1\n# Location of the nodepool.\nregion: Germany West Central\n# Zone of the nodepool.\nzone: 2\ncount: 2\n# VM size name.\nserverType: Standard_B2s\n# URN of the image.\nimage: Canonical:0001-com-ubuntu-minimal-jammy:minimal-22_04-lts:22.04.202212120\nstorageDiskSize: 50\n\n- name: compute-azure-2\nproviderSpec:\n# Name of the provider instance.\nname: azure-2\n# Location of the nodepool.\nregion: West Europe\n# Zone of the nodepool.\nzone: 3\ncount: 2\n# VM size name.\nserverType: Standard_B2s\n# URN of the image.\nimage: Canonical:0001-com-ubuntu-minimal-jammy:minimal-22_04-lts:22.04.202212120\nstorageDiskSize: 50\n\nkubernetes:\nclusters:\n- name: azure-cluster\nversion: v1.23.0\nnetwork: 192.168.2.0/24\npools:\ncontrol:\n- control-azure-1\n- control-azure-2\ncompute:\n- compute-azure-1\n- compute-azure-2\n</code></pre>"},{"location":"input-manifest/providers/examples/gcp-input-manifest/","title":"GCP input manifest example","text":""},{"location":"input-manifest/providers/examples/gcp-input-manifest/#single-provider-multi-region-cluster","title":"Single provider, multi region cluster","text":"<pre><code>name: GCPExampleManifest\n\nproviders:\ngcp:\n- name: gcp-1\n# GCP project for the service account.\ngcpProject: project-claudie\n# Service account key.\ncredentials: |\n{\n\"type\": \"service_account\",\n\"project_id\": \"project-claudie\",\n\"private_key_id\": \"bskdlo875s9087394763eb84e407903lskdimp439\",\n\"private_key\": \"-----BEGIN PRIVATE KEY-----\\nSKLOosKJUSDANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhki\\n-----END PRIVATE KEY-----\\n\",\n\"client_email\": \"claudie@project-claudie-123456.iam.gserviceaccount.com\",\n\"client_id\": \"109876543211234567890\",\"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n\"token_uri\": \"https://oauth2.googleapis.com/token\",\n\"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n\"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/claudie%40claudie-project-123456.iam.gserviceaccount.com\"\n}\n\nnodePools:\ndynamic:\n- name: control-gcp\nproviderSpec:\n# Name of the provider instance.\nname: gcp-1\n# Region of the nodepool.\nregion: europe-west1\n# Zone of the nodepool.\nzone: europe-west1-c\ncount: 1\n# Machine type name.\nserverType: e2-medium\n# OS image name.\nimage: ubuntu-os-cloud/ubuntu-2204-jammy-v20221206\n\n- name: compute-1-gcp\nproviderSpec:\n# Name of the provider instance.\nname: gcp-1\n# Region of the nodepool.\nregion: europe-west3\n# Zone of the nodepool.\nzone: europe-west3-a\ncount: 2\n# Machine type name.\nserverType: e2-medium\n# OS image name.\nimage: ubuntu-os-cloud/ubuntu-2204-jammy-v20221206\nstorageDiskSize: 50\n\n- name: compute-2-gcp\nproviderSpec:\n# Name of the provider instance.\nname: gcp-1\n# Region of the nodepool.\nregion: europe-west2\n# Zone of the nodepool.\nzone: europe-west2-a\ncount: 2\n# Machine type name.\nserverType: e2-medium\n# OS image name.\nimage: ubuntu-os-cloud/ubuntu-2204-jammy-v20221206\nstorageDiskSize: 50\n\nkubernetes:\nclusters:\n- name: gcp-cluster\nversion: v1.23.0\nnetwork: 192.168.2.0/24\npools:\ncontrol:\n- control-gcp\ncompute:\n- compute-1-gcp\n- compute-2-gcp\n</code></pre>"},{"location":"input-manifest/providers/examples/gcp-input-manifest/#multi-provider-multi-region-clusters","title":"Multi provider, multi region clusters","text":"<pre><code>name: GCPExampleManifest\n\nproviders:\ngcp:\n- name: gcp-1\n# GCP project for the service account.\ngcpProject: project-claudie-1\n# Service account key.\ncredentials: |\n{\n\"type\": \"service_account\",\n\"project_id\": \"project-claudie-1\",\n\"private_key_id\": \"bskdlo875s9087394763eb84e407903lskdimp439\",\n\"private_key\": \"-----BEGIN PRIVATE KEY-----\\nSKLOosKJUSDANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhki\\n-----END PRIVATE KEY-----\\n\",\n\"client_email\": \"claudie@project-claudie-2-123456.iam.gserviceaccount.com\",\n\"client_id\": \"109876543211234567890\",\"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n\"token_uri\": \"https://oauth2.googleapis.com/token\",\n\"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n\"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/claudie%40claudie-project-123456.iam.gserviceaccount.com\"\n}\n- name: gcp-2\n# GCP project for the service account.\ngcpProject: project-claudie-1\n# Service account key.\ncredentials: |\n{\n\"type\": \"service_account\",\n\"project_id\": \"project-claudie-1\",\n\"private_key_id\": \"bskdlo875sregergsrh234b84e407903lskdimp439\",\n\"private_key\": \"-----BEGIN PRIVATE KEY-----\\nSKLOosKJUSDANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\\nMIIEvQIBADANBgkqhki\\n-----END PRIVATE KEY-----\\n\",\n\"client_email\": \"claudie@project-claudie-2-45y342.iam.gserviceaccount.com\",\n\"client_id\": \"4566523462523454352435\",\"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n\"token_uri\": \"https://oauth2.googleapis.com/token\",\n\"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n\"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/claudie%40claudie-project-123456.iam.gserviceaccount.com\"\n}\n\nnodePools:\ndynamic:\n- name: control-gcp-1\nproviderSpec:\n# Name of the provider instance.\nname: gcp-1\n# Region of the nodepool.\nregion: europe-west1\n# Zone of the nodepool.\nzone: europe-west1-c\ncount: 1\n# Machine type name.\nserverType: e2-medium\n# OS image name.\nimage: ubuntu-os-cloud/ubuntu-2204-jammy-v20221206\n\n- name: control-gcp-2\nproviderSpec:\n# Name of the provider instance.\nname: gcp-2\n# Region of the nodepool.\nregion: europe-west1\n# Zone of the nodepool.\nzone: europe-west1-a\ncount: 2\n# Machine type name.\nserverType: e2-medium\n# OS image name.\nimage: ubuntu-os-cloud/ubuntu-2204-jammy-v20221206\n\n- name: compute-gcp-1\nproviderSpec:\n# Name of the provider instance.\nname: gcp-1\n# Region of the nodepool.\nregion: europe-west3\n# Zone of the nodepool.\nzone: europe-west3-a\ncount: 2\n# Machine type name.\nserverType: e2-medium\n# OS image name.\nimage: ubuntu-os-cloud/ubuntu-2204-jammy-v20221206\nstorageDiskSize: 50\n\n- name: compute-gcp-2\nproviderSpec:\n# Name of the provider instance.\nname: gcp-2\n# Region of the nodepool.\nregion: europe-west1\n# Zone of the nodepool.\nzone: europe-west1-c\ncount: 2\n# Machine type name.\nserverType: e2-medium\n# OS image name.\nimage: ubuntu-os-cloud/ubuntu-2204-jammy-v20221206\nstorageDiskSize: 50\n\nkubernetes:\nclusters:\n- name: gcp-cluster\nversion: v1.23.0\nnetwork: 192.168.2.0/24\npools:\ncontrol:\n- control-gcp-1\n- control-gcp-2\ncompute:\n- compute-gcp-1\n- compute-gcp-2\n</code></pre>"},{"location":"input-manifest/providers/examples/hetzner-input-manifest/","title":"Hetzner input manifest example","text":""},{"location":"input-manifest/providers/examples/hetzner-input-manifest/#single-provider-multi-region-cluster","title":"Single provider, multi region cluster","text":"<pre><code>name: HetznerExampleManifest\n\nproviders:\nhetzner:\n- name: hetzner-1\n# API access token.\ncredentials: kslISA878a6etYAfXYcg5iYyrFGNlCxcICo060HVEygjFs21nske76ksjKko21lp\n\nnodePools:\ndynamic:\n- name: control-hetzner\nproviderSpec:\n# Name of the provider instance.\nname: hetzner-1\n# Region of the nodepool.\nregion: hel1\n# Datacenter of the nodepool.\nzone: hel1-dc2\ncount: 1\n# Machine type name.\nserverType: cpx11\n# OS image name.\nimage: ubuntu-22.04\n\n- name: compute-1-hetzner\nproviderSpec:\n# Name of the provider instance.\nname: hetzner-1\n# Region of the nodepool.\nregion: fsn1\n# Datacenter of the nodepool.\nzone: fsn1-dc14\ncount: 2\n# Machine type name.\nserverType: cpx11\n# OS image name.\nimage: ubuntu-22.04\nstorageDiskSize: 50\n\n- name: compute-2-hetzner\nproviderSpec:\n# Name of the provider instance.\nname: hetzner-1\n# Region of the nodepool.\nregion: nbg1\n# Datacenter of the nodepool.\nzone: nbg1-dc3\ncount: 2\n# Machine type name.\nserverType: cpx11\n# OS image name.\nimage: ubuntu-22.04\nstorageDiskSize: 50\n\nkubernetes:\nclusters:\n- name: hetzner-cluster\nversion: v1.23.0\nnetwork: 192.168.2.0/24\npools:\ncontrol:\n- control-hetzner\ncompute:\n- compute-1-hetzner\n- compute-2-hetzner\n</code></pre>"},{"location":"input-manifest/providers/examples/hetzner-input-manifest/#multi-provider-multi-region-clusters","title":"Multi provider, multi region clusters","text":"<pre><code>name: HetznerExampleManifest\n\nproviders:\nhetzner:\n- name: hetzner-1\n# API access token.\ncredentials: kslISA878a6etYAfXYcg5iYyrFGNlCxcICo060HVEygjFs21nske76ksjKko21lp\n\n- name: hetzner-2\n# API access token.\ncredentials: kslIIOUYBiuui7iGBYIUiuybpiUB87bgPyuCo060HVEygjFs21nske76ksjKko21l\n\nnodePools:\ndynamic:\n- name: control-hetzner-1\nproviderSpec:\n# Name of the provider instance.\nname: hetzner-1\n# Region of the nodepool.\nregion: hel1\n# Datacenter of the nodepool.\nzone: hel1-dc2\ncount: 1\n# Machine type name.\nserverType: cpx11\n# OS image name.\nimage: ubuntu-22.04\n\n- name: control-hetzner-2\nproviderSpec:\n# Name of the provider instance.\nname: hetzner-2\n# Region of the nodepool.\nregion: fsn1\n# Datacenter of the nodepool.\nzone: fsn1-dc14\ncount: 2\n# Machine type name.\nserverType: cpx11\n# OS image name.\nimage: ubuntu-22.04\n\n- name: compute-hetzner-1\nproviderSpec:\n# Name of the provider instance.\nname: hetzner-1\n# Region of the nodepool.\nregion: fsn1\n# Datacenter of the nodepool.\nzone: fsn1-dc14\ncount: 2\n# Machine type name.\nserverType: cpx11\n# OS image name.\nimage: ubuntu-22.04\nstorageDiskSize: 50\n\n- name: compute-hetzner-2\nproviderSpec:\n# Name of the provider instance.\nname: hetzner-2\n# Region of the nodepool.\nregion: nbg1\n# Datacenter of the nodepool.\nzone: nbg1-dc3\ncount: 2\n# Machine type name.\nserverType: cpx11\n# OS image name.\nimage: ubuntu-22.04\nstorageDiskSize: 50\n\nkubernetes:\nclusters:\n- name: hetzner-cluster\nversion: v1.23.0\nnetwork: 192.168.2.0/24\npools:\ncontrol:\n- control-hetzner-1\n- control-hetzner-2\ncompute:\n- compute-hetzner-1\n- compute-hetzner-2\n</code></pre>"},{"location":"input-manifest/providers/examples/oci-input-manifest/","title":"OCI input manifest example","text":""},{"location":"input-manifest/providers/examples/oci-input-manifest/#single-provider-multi-region-cluster","title":"Single provider, multi region cluster","text":"<pre><code>name: OCIExampleManifest\n\nproviders:\noci:\n- name: oci-1\n# Private key to the user account.\nprivateKey: |\n-----BEGIN RSA PRIVATE KEY-----\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/==\n-----END RSA PRIVATE KEY-----\n# Fingerprint of the key pair.\nkeyFingerprint: ab:cd:3f:34:33:22:32:34:54:54:45:76:76:78:98:aa\n# OCID of the tenancy.\ntenancyOcid: ocid2.tenancy.oc2..aaaaaaaayrsfvlvxc34o060kfdygsds21nske76ksjkko21lpsdfsfsgbrtghs\n# OCID of the user.\nuserOcid: ocid2.user.oc2..aaaaaaaaaanyrsfvlvxc34o060kfdygsds21nske76ksjkko21lpsdfsf\n# OCID of the compartment.\ncompartmentOcid: ocid2.compartment.oc2..aaaaaaaaa2rsfvlvxc34o060kfdygsds21nske76ksjkko21lpsdfsf\n\nnodePools:\ndynamic:\n- name: control-oci\nproviderSpec:\n# Name of the provider instance.\nname: oci-1\n# Region of the nodepool.\nregion: eu-milan-1\n# Availability domain of the nodepool.\nzone: hsVQ:EU-MILAN-1-AD-1\ncount: 1\n# VM shape name.\nserverType: VM.Standard2.2\n# OCID of the image.\n# Make sure to update it according to the region.\nimage: ocid1.image.oc1.eu-frankfurt-1.aaaaaaaavvsjwcjstxt4sb25na65yx6i34bzdy5oess3pkgwyfa4hxmzpqeq\n\n- name: compute-1-oci\nproviderSpec:\n# Name of the provider instance.\nname: oci-1\n# Region of the nodepool.\nregion: eu-frankfurt-1\n# Availability domain of the nodepool.\nzone: hsVQ:EU-FRANKFURT-1-AD-1\ncount: 2\n# VM shape name.\nserverType: VM.Standard2.1\n# OCID of the image.\n# Make sure to update it according to the region.\nimage: ocid1.image.oc1.eu-frankfurt-1.aaaaaaaavvsjwcjstxt4sb25na65yx6i34bzdy5oess3pkgwyfa4hxmzpqeq\nstorageDiskSize: 50\n\n- name: compute-2-oci\nproviderSpec:\n# Name of the provider instance.\nname: oci-1\n# Region of the nodepool.\nregion: eu-frankfurt-1\n# Availability domain of the nodepool.\nzone: hsVQ:EU-FRANKFURT-1-AD-2\ncount: 2\n# VM shape name.\nserverType: VM.Standard2.1\n# OCID of the image.\n# Make sure to update it according to the region.\nimage: ocid1.image.oc1.eu-frankfurt-1.aaaaaaaavvsjwcjstxt4sb25na65yx6i34bzdy5oess3pkgwyfa4hxmzpqeq\nstorageDiskSize: 50\n\nkubernetes:\nclusters:\n- name: oci-cluster\nversion: v1.23.0\nnetwork: 192.168.2.0/24\npools:\ncontrol:\n- control-oci\ncompute:\n- compute-1-oci\n- compute-2-oci\n</code></pre>"},{"location":"input-manifest/providers/examples/oci-input-manifest/#multi-provider-multi-region-clusters","title":"Multi provider, multi region clusters","text":"<pre><code>name: OCIExampleManifest\n\nproviders:\noci:\n- name: oci-1\n# Private key to the user account.\nprivateKey: |\n-----BEGIN RSA PRIVATE KEY-----\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/askJSLosad\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/==\n-----END RSA PRIVATE KEY-----\n# Fingerprint of the key pair.\nkeyFingerprint: ab:cd:3f:34:33:22:32:34:54:54:45:76:76:78:98:aa\n# OCID of the tenancy.\ntenancyOcid: ocid2.tenancy.oc2..aaaaaaaayrsfvlvxc34o060kfdygsds21nske76ksjkko21lpsdfsfsgbrtghs\n# OCID of the user.\nuserOcid: ocid2.user.oc2..aaaaaaaaaanyrsfvlvxc34o060kfdygsds21nske76ksjkko21lpsdfsf\n# OCID of the compartment.\ncompartmentOcid: ocid2.compartment.oc2..aaaaaaaaa2rsfvlvxc34o060kfdygsds21nske76ksjkko21lpsdfsf\n\n- name: oci-2\n# Private key to the user account.\nprivateKey: |\n-----BEGIN RSA PRIVATE KEY-----\nIUBJNINoisdncNIUBNNpniuniupNPIUNuipbnPIUNPIUBSNUPIbnui/OUINNPOIn\nIUBJNINoisdncNIUBNNpniuniupNPIUNuipbnPIUNPIUBSNUPIbnui/OUINNPOIn\nIUBJNINoisdncNIUBNNpniuniupNPIUNuipbnPIUNPIUBSNUPIbnui/OUINNPOIn\nIUBJNINoisdncNIUBNNpniuniupNPIUNuipbnPIUNPIUBSNUPIbnui/OUINNPOIn\nIUBJNINoisdncNIUBNNpniuniupNPIUNuipbnPIUNPIUBSNUPIbnui/OUINNPOIn\nIUBJNINoisdncNIUBNNpniuniupNPIUNuipbnPIUNPIUBSNUPIbnui/OUINNPOIn\nIUBJNINoisdncNIUBNNpniuniupNPIUNuipbnPIUNPIUBSNUPIbnui/OUINNPOIn\nIUBJNINoisdncNIUBNNpniuniupNPIUNuipbnPIUNPIUBSNUPIbnui/OUINNPOIn\nIUBJNINoisdncNIUBNNpniuniupNPIUNuipbnPIUNPIUBSNUPIbnui/OUINNPOIn\nIUBJNINoisdncNIUBNNpniuniupNPIUNuipbnPIUNPIUBSNUPIbnui/OUINNPOIn\nIUBJNINoisdncNIUBNNpniuniupNPIUNuipbnPIUNPIUBSNUPIbnui/OUINNPOIn\nIUBJNINoisdncNIUBNNpniuniupNPIUNuipbnPIUNPIUBSNUPIbnui/OUINNPOIn\nIUBJNINoisdncNIUBNNpniuniupNPIUNuipbnPIUNPIUBSNUPIbnui/OUINNPOIn\nIUBJNINoisdncNIUBNNpniuniupNPIUNuipbnPIUNPIUBSNUPIbnui/OUINNPOIn\nIUBJNINoisdncNIUBNNpniuniupNPIUNuipbnPIUNPIUBSNUPIbnui/OUINNPOIn\nIUBJNINoisdncNIUBNNpniuniupNPIUNuipbnPIUNPIUBSNUPIbnui/OUINNPOIn\nIUBJNINoisdncNIUBNNpniuniupNPIUNuipbnPIUNPIUBSNUPIbnui/OUINNPOIn\nIUBJNINoisdncNIUBNNpniuniupNPIUNuipbnPIUNPIUBSNUPIbnui/OUINNPOIn\nIUBJNINoisdncNIUBNNpniuniupNPIUNuipbnPIUNPIUBSNUPIbnui/OUINNPOIn\nIUBJNINoisdncNIUBNNpniuniupNPIUNuipbnPIUNPIUBSNUPIbnui/OUINNPOIn\nIUBJNINoisdncNIUBNNpniuniupNPIUNuipbnPIUNPIUBSNUPIbnui/OUINNPOIn\nIUBJNINoisdncNIUBNNpniuniupNPIUNuipbnPIUNPIUBSNUPIbnui/OUINNPOIn\nIUBJNINoisdncNIUBNNpniuniupNPIUNuipbnPIUNPIUBSNUPIbnui/OUINNPOIn\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCj2/==\n-----END RSA PRIVATE KEY-----\n# Fingerprint of the key pair.\nkeyFingerprint: 34:54:54:45:76:76:78:98:aa:ab:cd:3f:34:33:22:32\n# OCID of the tenancy.\ntenancyOcid: ocid2.tenancy.oc2..aaaaaaaayreragzafbdrfedbfdagagrgregagrrgaregfdgvrehdfsfsgbrtghs\n# OCID of the user.\nuserOcid: ocid2.user.oc2..aaaaaaaaaanyrsfvlvxc3argaehgaergaregraregaregarsdfsfrgreg2ds\n# OCID of the compartment.\ncompartmentOcid: ocid2.compartment.oc2..aaaaaaaaa2rsfvlvxc3argaregaregraegzfgragfksjkko21lpsdfsf\n\nnodePools:\ndynamic:\n- name: control-oci-1\nproviderSpec:\n# Name of the provider instance.\nname: oci-1\n# Region of the nodepool.\nregion: eu-milan-1\n# Availability domain of the nodepool.\nzone: hsVQ:EU-MILAN-1-AD-1\ncount: 1\n# VM shape name.\nserverType: VM.Standard2.2\n# OCID of the image.\n# Make sure to update it according to the region.\nimage: ocid1.image.oc1.eu-frankfurt-1.aaaaaaaavvsjwcjstxt4sb25na65yx6i34bzdy5oess3pkgwyfa4hxmzpqeq\n\n- name: control-oci-2\nproviderSpec:\n# Name of the provider instance.\nname: oci-2\n# Region of the nodepool.\nregion: eu-frankfurt-1\n# Availability domain of the nodepool.\nzone: hsVQ:EU-FRANKFURT-1-AD-3\ncount: 2\n# VM shape name.\nserverType: VM.Standard2.1\n# OCID of the image.\n# Make sure to update it according to the region.\nimage: ocid1.image.oc1.eu-frankfurt-1.aaaaaaaavvsjwcjstxt4sb25na65yx6i34bzdy5oess3pkgwyfa4hxmzpqeq\n\n- name: compute-oci-1\nproviderSpec:\n# Name of the provider instance.\nname: oci-1\n# Region of the nodepool.\nregion: eu-frankfurt-1\n# Availability domain of the nodepool.\nzone: hsVQ:EU-FRANKFURT-1-AD-1\ncount: 2\n# VM shape name.\nserverType: VM.Standard2.1\n# OCID of the image.\n# Make sure to update it according to the region.\nimage: ocid1.image.oc1.eu-frankfurt-1.aaaaaaaavvsjwcjstxt4sb25na65yx6i34bzdy5oess3pkgwyfa4hxmzpqeq\nstorageDiskSize: 50\n\n- name: compute-oci-2\nproviderSpec:\n# Name of the provider instance.\nname: oci-2\n# Region of the nodepool.\nregion: eu-milan-1\n# Availability domain of the nodepool.\nzone: hsVQ:EU-MILAN-1-AD-1\ncount: 2\n# VM shape name.\nserverType: VM.Standard2.1\n# OCID of the image.\n# Make sure to update it according to the region..\nimage: ocid1.image.oc1.eu-frankfurt-1.aaaaaaaavvsjwcjstxt4sb25na65yx6i34bzdy5oess3pkgwyfa4hxmzpqeq\nstorageDiskSize: 50\n\nkubernetes:\nclusters:\n- name: oci-cluster\nversion: v1.23.0\nnetwork: 192.168.2.0/24\npools:\ncontrol:\n- control-oci-1\n- control-oci-2\ncompute:\n- compute-oci-1\n- compute-oci-2\n</code></pre>"},{"location":"loadbalancing/loadbalancing-solution/","title":"Claudie load balancing solution","text":""},{"location":"loadbalancing/loadbalancing-solution/#loadbalancer","title":"Loadbalancer","text":"<p>To create a highly available kubernetes cluster, Claudie creates load balancers for the <code>kubeAPI</code> server. These load balancers use Nginx to load balance the traffic among the cluster nodes. Claudie also supports definition of custom load balancers for the applications running inside the cluster.</p>"},{"location":"loadbalancing/loadbalancing-solution/#concept","title":"Concept","text":"<ul> <li> <p>The load balancer machines will join the Wireguard private network of Claudie clusters relevant to it.</p> <ul> <li>This is necessary so that the LB machines can send traffic to the cluster machines over the <code>wireguard VPN</code>.</li> </ul> </li> <li> <p>DNS A records will be created and managed by Claudie on 1 or more cloud providers.</p> <ul> <li>There will be a DNS A record for the public IP of each LB machine that is currently passing the health checks.</li> </ul> </li> <li> <p>The LB machines will run an <code>Nginx</code> to carry out the actual load balancing.</p> <ul> <li>There will be a DNS A record for the public IP of each LB machine that is currently passing the health checks.</li> <li>Therefore, there will be actually 2 layers of load balancing.<ol> <li>DNS-based load balancing to determine the LB machine to be used.</li> <li>Software load balancing on the chosen LB machine.</li> </ol> </li> </ul> </li> <li> <p>Claudie will dynamically manage the LB configuration, e.g. if some cluster node is removed, the LB configuration changes or DNS configuration changes (hostname change).</p> </li> <li> <p>The load balancing will be on L4 layer, TCP/UDP, partially configurable by the Claudie input manifest.</p> </li> </ul>"},{"location":"loadbalancing/loadbalancing-solution/#example-diagram","title":"Example diagram","text":""},{"location":"loadbalancing/loadbalancing-solution/#definitions","title":"Definitions","text":""},{"location":"loadbalancing/loadbalancing-solution/#role","title":"Role","text":"<p>Claudie uses the concept of roles while configuring the load balancers from the input manifest. Each role represents a loadbalancer configuration for a particular use. Roles are then assigned to the load balancer cluster. A single load balancer cluster can have multiple roles assigned.</p>"},{"location":"loadbalancing/loadbalancing-solution/#targeted-kubernetes-cluster","title":"Targeted kubernetes cluster","text":"<p>Load balancer gets assigned to a kubernetes cluster with the field <code>targetedK8s</code>. This field is using the <code>name</code> of the kubernetes cluster as a value. Currently, a single load balancer can only be assigned to a single kubernetes cluster.</p> <p>Among multiple load balancers targeting the same kubernetes cluster only one of them can have the API server role (i.e. the role with target port 6443) attached to it.</p>"},{"location":"loadbalancing/loadbalancing-solution/#dns","title":"DNS","text":"<p>Claudie creates and manages the DNS for the load balancer. If the user adds a load balancer into their infrastructure via Claudie, Claudie creates a DNS A record with the public IP of the load balancer machines behind it. When the load balancer configuration changes in any way, that is a node is added/removed, the hostname or the target changes, the DNS record is reconfigured by Claudie on the fly. This rids the user of the need to manage DNS.</p>"},{"location":"loadbalancing/loadbalancing-solution/#nodepools","title":"Nodepools","text":"<p>Loadbalancers are build from user defined nodepools in <code>pools</code> field, similar to how kubernetes clusters are defined. These nodepools allow the user to change/scale the load balancers according to their needs without any fuss. See the nodepool definition for more information.</p>"},{"location":"loadbalancing/loadbalancing-solution/#an-example-of-load-balancer-definition","title":"An example of load balancer definition","text":"<p>See an example load balancer definition in our reference example input manifest.</p>"},{"location":"roadmap/roadmap/","title":"Roadmap for Claudie","text":"<ul> <li> Support for more cloud providers<ul> <li> OCI</li> <li> AWS</li> <li> Azure</li> <li> Cloudflare</li> </ul> </li> <li> Periodic health checks &amp; sync loops for the managed clusters</li> <li> Hybrid-cloud support (on-premises)</li> <li> <code>arm64</code> support for the nodepools</li> <li> Support for Spot instances</li> <li> Autoscaler</li> </ul>"},{"location":"storage/storage-solution/","title":"Claudie storage solution","text":""},{"location":"storage/storage-solution/#concept","title":"Concept","text":"<p>Running stateful workloads is a complex task, even more so when considering the multi-cloud environment. Claudie therefore needs to be able to accommodate stateful workloads, regardless of the underlying infrastructure providers.</p> <p>Claudie orchestrates storage on the kubernetes cluster nodes by creating one \"storage cluster\" across multiple providers. This \"storage cluster\" has a series of <code>zones</code>, one for each cloud provider instance. Each <code>zone</code> then stores its own persistent volume data.</p> <p>This concept is translated into longhorn implementation, where each <code>zone</code> is represented by a Storage Class which is backed up by the nodes defined under the same cloud provider instance. Furthermore, each node uses separate disk to the one, where OS is installed, to assure clear data separation. The size of the storage disk can be configured in <code>storageDiskSize</code> field of the nodepool specification.</p>"},{"location":"storage/storage-solution/#longhorn","title":"Longhorn","text":"<p>A Claudie-created cluster comes with the <code>longhorn</code> deployment preinstalled and ready to be used. By default, only worker nodes are used to store data.</p> <p>Longhorn installed in the cluster is set up in a way that it provides one default <code>StorageClass</code> called <code>longhorn</code>, which, if used, creates a volume that is then replicated across random nodes in the cluster.</p> <p>Besides the default storage class, Claudie can also create custom storage classes, which force persistent volumes to be created on specific nodes based on the provider instance they have. In other words, you can use a specific provider instance to provision nodes for your storage needs, while using another provider instance for computing tasks.</p>"},{"location":"storage/storage-solution/#example","title":"Example","text":"<p>To follow along, have a look at the reference example input manifest file.</p> <p>When Claudie applies this input manifest, the following storage classes are installed:</p> <ul> <li><code>longhorn</code> - the default storage class, which stores data on random nodes</li> <li><code>longhorn-&lt;provider instance&gt;-zone</code> - storage class, which stores data only on nodes of the specific provider instance (see the list of supported providers), i.e. <code>longhorn-gcp-1-zone</code>, <code>longhorn-gcp-2-zone</code>, <code>longhorn-aws-1-zone</code>, ...</li> </ul> <p>For more information on how Longhorn works you can check out Longhorn's official documentation.</p>"},{"location":"use-cases/use-cases/","title":"Use-cases and customers","text":"<p>We foresee the following use-cases of the Claudie platform</p>"},{"location":"use-cases/use-cases/#1-cloud-bursting","title":"1. Cloud-bursting","text":"<p>A company uses advanced cloud features in one of the hyper-scale providers (e.g. serverless Lambda and API Gateway functionality in AWS). They run a machine-learning application that they need to train for a pattern on a dataset. The learning phase requires significant compute resources. Claudie allows to extend the cluster in AWS (needed in order to access the AWS functionality) to Hetzner for saving the infrastructure costs of the machine-learning case.</p> <p>Typical client profiles:</p> <ul> <li>startups</li> <li>in need of significant computing power already in their early stages (e.g. AI/ML workloads)</li> </ul>"},{"location":"use-cases/use-cases/#2-cost-saving","title":"2. Cost-saving","text":"<p>A company would like to utilize their on-premise or leased resources that they already invested into, but would like to:</p> <ol> <li>extend the capacity</li> <li>access managed features of a hyper-scale provider (AWS, GCP, ...)</li> <li>get the workload physically closer to a client (e. g. to South America)</li> </ol> <p>Typical client profile:</p> <ul> <li>medium-size business</li> <li>possibly already familiar with containerized workload</li> </ul>"},{"location":"use-cases/use-cases/#3-smart-layer-as-a-service-on-top-of-simple-cloud-providers","title":"3. Smart-layer-as-a-Service on top of simple cloud-providers","text":"<p>An existing customer of medium-size provider (e.g. Exoscale) would like to utilize features that are typical for hyper-scale providers. Their current provider does neither offer nor plan to offer such an advanced functionality.</p> <p>Typical client profile:</p> <ul> <li>established business</li> <li>need to access advanced managed features to innovate faster</li> </ul>"},{"location":"use-cases/use-cases/#4-service-interconnect","title":"4. Service interconnect","text":"<p>A company would like to access on-premise-hosted services and cloud-managed services from within the same cluster. For on-premise services the on-premise cluster node would egress the traffic. The cloud-hosted cluster nodes would deal with the egress traffic to the cloud-managed services.</p> <p>Typical client profile:</p> <ul> <li>medium-size/established business</li> <li>already contains on-premise workloads</li> <li>has the need to take the advantage of managed cloud infra (from cost, agility, or capacity reasons)</li> </ul>"}]}